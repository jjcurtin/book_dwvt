[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Wrangling and Visualization in the Tidyverse",
    "section": "",
    "text": "Welcome\nJohn Curtin and his students and staff at the Addiction Research Center are developing this book to document what we believe to be best practices (or at least our practices) for data wrangling and related tasks using the Tidyverse and Tidymodels ecosystems and Quarto publishing system.\nThis book is under active development with its most recent edits on r lubridate::today(). We hope that this resource will continue to evolve to serve both our lab and the R community more generally.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "conflicts.html",
    "href": "conflicts.html",
    "title": "1  Function conflicts",
    "section": "",
    "text": "1.1 Minmize loading of full packages\nFunction conflicts can be minimized by limiting the number of packages that you attach in your scripts. For our work, we will almost always use library(tidyverse) and frequently use library(tidymodels) next. You should carefully consider if you need to attach any other packages. You very well may not!\nYou should only attach full packages if you intend to use multiple functions from that package. If instead, you only need a single function (or several), there are two alternatives that are preferred over attaching the full package with library().\nOption 1: Use the namespace of the function when calling it in your script. For example, if I need to simulate multivariate normal data, I might want to use the mvrnorm() function from the MASS package. I do NOT need to use library(MASS) to use this function. Instead, I can simply call the function with its namespace MASS::mvrnorm(). This will avoid conflicts between other functions in MASS and your other attached packages (e.g., select() in MASS conflicts with select() in dplyr/tidyverse).\nOption 2: If you find using the namespace of the function cumbersome, you can attach a single function from a package rather than the full package. For example, if we wanted to use only mvrnorm() from MASS, we could use this code: library(MASS, include.only = \"mvrnorm). Now you can call mvrnorm() without pre-pending its namespace (MASS::). You can pass a character vector containing multiple function names rather than a single function to include.only if you intend to use several functions from the package (e.g., library(MASS, include.only = c(\"mvrnorm\", \"lda\"))).\nOption 3: If you really need to load most of the functions in a package but several conflict with tidyverse or tidymodels (or other key packages), you can load the additional package but exclude the problematic function or functions. For example, if you really want to use many of the functions in the MASS package, you could load MASS but exclude the select() function to avoid that conflict: library(MASS, exclude = c(\"select\")). If you later need to use select() from MASS, you can use the namespace MASS::select() to call it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Function conflicts</span>"
    ]
  },
  {
    "objectID": "conflicts.html#base-r-conflict-managemnt",
    "href": "conflicts.html#base-r-conflict-managemnt",
    "title": "1  Function conflicts",
    "section": "1.2 Base R conflict managemnt",
    "text": "1.2 Base R conflict managemnt\nAs of version 3.6, R now includes all the necessary tools (in our opinion) to handle and clearly resolve function conflicts. These tools are well-documented and should be reviewed to better understand how to use them.\nFor our purposes, it is generally sufficient to use one of the two named conflict policies that are included (depends.ok or strict). We prefer the use of the depends.ok policy.\nTo implement the depends.ok policy, simply set this option near the top of your script (more on why “near” in a moment) using options(conflicts.policy = \"depends.ok\"). You can now combine this option with limited use of library() for important packages and the use of either namespace or include.only methods described above and you should be good to go (with one clarification noted below).\nTo get a better sense of what the depends.ok policy does, it is a shortcut to implement the following set of conflict options.\n\noptions(conflicts.policy =\n            list(error = TRUE,\n                 generics.ok = TRUE,\n                 can.mask = c(\"base\", \"methods\", \"utils\",\n                              \"grDevices\", \"graphics\",\n                              \"stats\"),\n                 depends.ok = TRUE))\n\nThis means that packages that you attach with library will produce an error if their functions conflict with previously loaded packages (error = TRUE).\nHowever, errors will not occur if the functions conflict with functions in base R (i.e., base R packages are listed in can.mask =) or S4 generic versions (generics.ok = TRUE). These exceptions generally make sense because package functions are often explicitly intended to mask or extend these functions.\nAn error will also not be produced if function conflicts exist within a single package (depends.ok = TRUE) because the package creator typically intended this as well.\nErrors due to other function conflicts will happen immediately when you try to load the new package so you can address these conflicts up front (by either including only a subset of functions from the library or using the function’s namespace instead).\nThere are more advanced tools to handle conflicts in special cases that are also described in the documentation but they should rarely be necessary.\nThere is one more detail that affects our common use of both tidyverse and tidymodels collections of packages. The depends.ok policy allows for conflicts/masking within a package. However, there are a couple of instances where some of the functions in tidymodels will conflict/mask functions in packages within tidyverse. R doesn’t recognize these as the same package so these conflicts will not be allowed if you try to these two packages after you set up your conflict policy.\nThe solution to this problem is simple. Load these two packages first and then set up the conflict policy afterward. This will protect you against any conflict with other packages, but will allow both tidyverse and tidymodels to load all their functions with now errors.\n\n# Load our two most important packages first\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Now xet up conflict policy to prevent other conflicts\noptions(conflicts.policy = \"depends.ok\")\n\n# Load the rest of your packages\nlibrary(broom)  # an example of loading a full package\nlibrary(janitor, include.only = \"clean_names\") # loading on key function from package\n\nAnd if you later decided you wanted to simulate data you could use the appropriate MASS function directly with its namespace. Notice use of mvrnorm() with namespace because we did not attach the MASS package. Notice use of clean_names() without namespace because we attached just that function for janitor package.\n\nd &lt;- MASS::mvrnorm(n = 10, mu = c(0,0), Sigma = diag(2)) %&gt;% \n  as_tibble(.name_repair = \"minimal\") %&gt;% \n  clean_names(\"snake\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Function conflicts</span>"
    ]
  },
  {
    "objectID": "conflicts.html#conflicted-package",
    "href": "conflicts.html#conflicted-package",
    "title": "1  Function conflicts",
    "section": "1.3 conflicted package",
    "text": "1.3 conflicted package\nYou should be aware that an alternative solution to handling function conflicts is provided in the conflicted package. However, this is no longer our preferred solution as the base R conflict policies are sufficient (so why load another package!). We also prefer to have conflicts detected immediately (when packages are attached) rather than at some later point when we call the function. The conflicted package is also less customizable than the base R polices.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Function conflicts</span>"
    ]
  },
  {
    "objectID": "cache.html",
    "href": "cache.html",
    "title": "2  Cache",
    "section": "",
    "text": "2.1 Solutions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cache</span>"
    ]
  },
  {
    "objectID": "cache.html#solutions",
    "href": "cache.html#solutions",
    "title": "2  Cache",
    "section": "",
    "text": "2.1.1 cache = TRUE (not recommended)\nYou can set cache = TRUE in any specific code chunk to have knitr cache those calculations for your later reuse. However, we don’t recommend this because it makes the process and instances where the cache is invalidated more opaque. And more importantly, this caching will not be used for interactive use when you send your code chunks to the console as you work live.\nNonetheless, this approach is well documented including more advanced topics like paths and lazy loading.\n\n\n2.1.2 Explicit write_rds() (OK - we use this in our lab at times)\nYou could instead manually save objects that you want to avoid recalculating. This is a legitimate method that gives you full and transparent control over caching. It will also work both interactively in the console and when you knit your document. However, its got a bit more overhead RE the code. You need to write code to check if the file exists and load it if it does vs. calculate the object if it doesn’t. This is not too hard but it turns out that a function has already been written to handle this overhead for you. We describe that next.\nHere is an example of this brute force (but more transparent) optioni\nMake a folder called cache at the root of your project. This is where we will store the file that contains the output of your costly computations.\nThen write a if-else statement that checks if that file already exist. If it does, load it. If it does not, do the computations and save the object (so that its available the next time to use)\nNow, if you need to update the computations, you must manually delete brute_force_method.rds and then the computations will be recalculated\n\nif (file.exists(here::here(\"cache/brute_force_method.rds\"))){\n  \n # file with computations exists, so just load it\n results &lt;- readr::read_rds(here::here(\"cache/brute_force_method.rds\"))\n  \n} else {\n \n  # pretend these computations take a while by calling Sys.sleep ()\n  Sys.sleep(10)\n  results &lt;- 1 \n \n  # write results so we have it the next time we run this code chunk \n  results |&gt;  readr::write_rds(here::here(\"cache/brute_force_method.rds\"))\n}\n\nresults\n\n[1] 1\n\n\n\n\n2.1.3 xfun::cache_rds() (our preferred method)\nWe believe that the xfun::cache_rds() function provides the sweet spot for the balance of control and transparency vs. code overhead. It also works for both interactive/console and render workflows.\nLets demonstrate its use.\n\nFirst we recommend setting up an environment variable called rerun_setting and setting it to FALSE. Put this with your other environment settings at the top of your script so you can find (and change) it easily\n\n\nrerun_setting &lt;- FALSE\n\nNext, we set up some objects that will be used in later time-consuming calculations. You need to be careful with these objects. If they ever change, you will need to explicitly invalidate your cached object and re-calculate it. More on that below.\n\ny &lt;- 2\nz &lt;- 3\n\nNow lets use y and z in some time consuming set of calculations\n\nThe first argument parameter in cache_rds() is the code to execute the time-consuming calculation. This code is provided to the function inside of curly brackets, {}\nResults from cache_rds() are assigned to your object (e.g., x) as if they came straight from the coded calculations (e.g., instead of x &lt;- y + z, we now have x &lt;- cache_rds({y + z})).\nWe recommend explicitly setting the values for the dir and file for the cached object. This way, you control where it is saved and are assured it will be the same location regardless of whether you run this code chunk in the console or knit it. Initial testing suggested the filename and location will differ for interactive/console vs. rendered workflows if you use defaults. The / at the end of the directory name is required to designate this as a folder. The filename will have the string assigned to file as the prefix but will have an additional hash and a .rds appended to it as well.\nWe recommend explicitly including rerun = rerun_setting as a third parameter. This provides you an easy way to invalidate the cached object (and a memory aid to consider invalidation when needed). To invalidate just this code chunk, set it to TRUE and run the chunk again again if any of your globals (e.g., y, z) have changed (and then set back to FALSE after!). We also recommended setting rerun_setting &lt;- TRUE when you are done with the script and ready to render a final version. This will fix any previously undetected cache issues in your final output.\n\ncache_rds() has one additional parameter worth mentioning, hash. You can pass a list of global objects to hash (e.g. hash = list(y, z)) that the function will monitor for change. If any of these globals are re-calculated, it will invalidate your cached object and re-calculate it. This could be very useful to catch cache invalidation issues. However, our testing suggests that it may invalidate the cache in some instances when it shouldn’t. We haven’t been able to fully document this issue yet. For now, we recommend not using this and instead manually invalidating as needed using either rerun = TRUE or rerun_setting &lt;- TRUE when you are done with your code.\n\n\nx &lt;- xfun::cache_rds({\n  Sys.sleep(5) # pretend that computations take a while\n  y + z\n},\nrerun = rerun_setting,\ndir = \"cache/\",\nfile = \"cache_demo\")\n\nNow we can use x without recalculating it each time when executing the previous chunk in either console or when knit. Yay!\n\nx\n\n[1] 5\n\n\nYou can (and should) read the full documentation on xfun::cache_rds() prior to using it in your own code.\n\n\n2.1.4 Final notes\n\n2.1.4.1 Cache and Github\nWe may not want our cached objects getting added to our repos. This could make the repos become too large and/or add sensitive data to them in some instance. It is easy to avoid this though. Assuming you are calling the folder that you will save the cached files to cache/ as recommended above, just add the following line to your .gitignore file\n*cache/\n\n\n2.1.4.2 Saving model objects\nWe have also learned that caching that involves saving an rds file (all of these methods) may encounter problems if you try to cache a keras model object (e.g., via mlp() in tidymodels). To be clear, there is no problem saving resampling statistics from fit_resamples() or tune_grid(). The problem is specific to the actual model object returned from fit(). This issue with keras (and perhaps some other types) models is documented and the bundles package is designed to solve it. If you plan to cache (or even just directly save) these model objects, read these docs carefully. We will eventually work out a piped solution that works to either manually save or use cache_rds() with these objects if needed. Not a high priority for us right now because we dont use keras much yet in our lab.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cache</span>"
    ]
  },
  {
    "objectID": "parallel_processing.html",
    "href": "parallel_processing.html",
    "title": "3  Parallel Processing",
    "section": "",
    "text": "3.0.1 future_map()\nHere is the use of map (that uses sequential processing)\ntic()\nx &lt;- map(c(2, 2, 2), \\(time) Sys.sleep(time))\ntoc()\n\n6.015 sec elapsed\nUsing future_map() without a plan (Don’t do this!)\ntic()\nx &lt;- future_map(c(2, 2, 2), \\(time) Sys.sleep(time))\ntoc()\n\n6.082 sec elapsed\nUsing future_map() with a plan (Do this!)\nplan(multisession, workers = parallel::detectCores(logical = FALSE))\ntic()\nx &lt;- future_map(c(2, 2, 2), \\(time) Sys.sleep(time))\ntoc()\n\n3.977 sec elapsed\nplan(sequential)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel Processing</span>"
    ]
  },
  {
    "objectID": "parallel_processing.html#tune_grid-in-tidymodels",
    "href": "parallel_processing.html#tune_grid-in-tidymodels",
    "title": "3  Parallel Processing",
    "section": "3.1 tune_grid() in tidymodels",
    "text": "3.1 tune_grid() in tidymodels\nSet up data, resamples, recipe, tuning grid. Will do 3x 10-fold CV to tune an elasticnet glm with a sample size of 1000 and 30 features\n\n# set up data\nn_obs &lt;- 1000\nn_x &lt;- 30\nirr_err &lt;- 5\nd &lt;- MASS::mvrnorm(n = n_obs, mu = rep(0,n_x), Sigma = diag(n_x)) %&gt;% \n    magrittr::set_colnames(str_c(\"x\", 1:n_x)) %&gt;% \n    as_tibble() %&gt;% \n    mutate(error = rnorm(n_obs, 0, irr_err),\n           y = rowSums(across(everything()))) %&gt;% \n    select(-error)\n\n# recipe\nrec &lt;- recipe(y ~ ., data = d)\n\n# 10-fold CV\nset.seed(19690127)\nsplits &lt;- d %&gt;% \n  vfold_cv(v = 10, strata = \"y\")\n\n# tuning grid\ntune_grid &lt;- expand_grid(penalty = exp(seq(0, 6, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 11))\n\nFirst, let’s benchmark without parallel processing. tune_grid() (and fit_resamples()) default is to allow parallel processing so have to explicitly turn it off using control_grid(). You will NOT do this. It is only to show the benefits of parallel processing.\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse),\n            control = control_grid(allow_par = FALSE)) # turn off pp\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 4\n   splits            id     .metrics             .notes          \n   &lt;list&gt;            &lt;chr&gt;  &lt;list&gt;               &lt;list&gt;          \n 1 &lt;split [900/100]&gt; Fold01 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [900/100]&gt; Fold02 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [900/100]&gt; Fold03 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [900/100]&gt; Fold04 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [900/100]&gt; Fold05 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [900/100]&gt; Fold06 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [900/100]&gt; Fold07 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [900/100]&gt; Fold08 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [900/100]&gt; Fold09 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [900/100]&gt; Fold10 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\ntoc()\n\n12.621 sec elapsed\n\n\nNow allow use of parallel processing (the default). No plan is needed here (consistent with findings for foreach()). Yay!\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse))\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 4\n   splits            id     .metrics             .notes          \n   &lt;list&gt;            &lt;chr&gt;  &lt;list&gt;               &lt;list&gt;          \n 1 &lt;split [900/100]&gt; Fold01 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [900/100]&gt; Fold02 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [900/100]&gt; Fold03 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [900/100]&gt; Fold04 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [900/100]&gt; Fold05 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [900/100]&gt; Fold06 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [900/100]&gt; Fold07 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [900/100]&gt; Fold08 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [900/100]&gt; Fold09 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [900/100]&gt; Fold10 &lt;tibble [2,200 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\ntoc()\n\n8.458 sec elapsed",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel Processing</span>"
    ]
  },
  {
    "objectID": "parallel_processing.html#final-notes",
    "href": "parallel_processing.html#final-notes",
    "title": "3  Parallel Processing",
    "section": "3.2 Final notes",
    "text": "3.2 Final notes\nThe following is often found as an alternative setup for a back-end for parallel processing. It works for future_map() (when combined with plan) and for foreach() but not in the tidymodels implementations of resampling. Not clear why since those use foreach() but this should not be used if you plan to use tidymodels resampling.\n\nlibrary(doFuture)\nregisterDoFuture()\n\nI tried this both directly and with various options of plan()\n\nplan(multisession, workers = parallel::detectCores(logical = FALSE))\n\nand with\n\ncl &lt;- makeCluster(parallel::detectCores(logical = FALSE))\nplan(cluster, workers = cl)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel Processing</span>"
    ]
  },
  {
    "objectID": "parallel_processing.html#conclusions",
    "href": "parallel_processing.html#conclusions",
    "title": "3  Parallel Processing",
    "section": "3.3 Conclusions",
    "text": "3.3 Conclusions\nFor future_map(), foreach(), and tidymodels functions in parallel, set up the parallel backend with this code chunk.\n\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\nNothing further is needed to use foreach() or tidymodels functions.\nFor future_map(), you need to set up a multisession plan with this code chunk\n\nplan(multisession, workers = parallel::detectCores(logical = FALSE))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel Processing</span>"
    ]
  },
  {
    "objectID": "file_and_path_management.html",
    "href": "file_and_path_management.html",
    "title": "4  File and Path Management",
    "section": "",
    "text": "4.1 Use of RStudio Projects\nThe use of RStudio Projects is critical to good managament of your paths and files. When you work within a project, you will have a working directory set within that project (based on where the project files is saved. This working directory can then be combined with relative paths for reading and writing data and other files. It also means that if you share the folders that contain your project (e.g., scripts, data), the paths will continue to work for that colleague as well, regardless of where they situate the folders on their computer.\nWickham et al., describe the rationale and benefits for using projects. Please read this! They also clearly describe the steps to set up a new project so I won’t repeat them here.\nFor our course, we strongly recommend that you set up a project called “iaml”. Inside that root project folder, you can establish a folder for “homework”, and inside that folder you can have sub-folders for each unit (e.g., “unit_2”, “unit_3”). In addition to the homework folder, you might have folders for exams (e.g., “midterm”) and other material that you save (e.g., “pdfs”).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>File and Path Management</span>"
    ]
  },
  {
    "objectID": "file_and_path_management.html#relative-paths",
    "href": "file_and_path_management.html#relative-paths",
    "title": "4  File and Path Management",
    "section": "4.2 Relative Paths",
    "text": "4.2 Relative Paths\nYou should also get in the habit of setting relative paths (relative to your project root) near the start of your script so that you can call those paths easily throughout. Added bonus, if you move those folders within your project, you just need to change one line of code. For example if your raw data and processed data live in separate folders you might have two paths set:\npath_raw &lt;- \"data/raw\"\npath_processed &lt;- \"data/processed\"\nYou can use these path objects with the base R function file.path()\nFor example, if you want to load a csv file in the folder that you indicated above by path_raw, you could use this line of code:\nd &lt;- read_csv(file.path(path_raw, \"raw_data.csv\"))\nalternatively, you could supply the relative path directly (though this is not preferred because it can be cumbersome if you move the folder later)\nd &lt;- read_csv(here(\"data/processed\", \"raw_data.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>File and Path Management</span>"
    ]
  },
  {
    "objectID": "file_and_path_management.html#reading-csv-files",
    "href": "file_and_path_management.html#reading-csv-files",
    "title": "4  File and Path Management",
    "section": "4.3 Reading csv files",
    "text": "4.3 Reading csv files\nWe typically save our data as csv files (with minor exceptions). There are many benefits to this format\n\neasy to share (colleagues don’t need R to access)\neasy to view outside of R (sometimes, we just want to see the data directly and csv can be viewed in any text editor and/or most spreadsheet apps) but one downside is that they don’t store information about variable/column class. We need to establish the appropriate class for each column when we read the data.\n\n\n4.3.1 Using col_types()\nIf possible, it is best to set the class for each column/variable specifically using the col_types() parameter in dplyr::read_csv() This forces you to specifically examine and consider each column to decide its class (e.g., is a column with numbers best set as numeric or ordered factor) and the levels if its class is nominal. Of course, this is part of cleaning EDA so you should have done this when you first started working with the data.\nRe-classing is typically needed to convert raw character columns to factor (ordered or unordered) and sometimes to convert raw numeric columns to factor (likely ordered, e.g., a likert scale).\nHere is an example using the cars dataset\n\npath_data &lt;- \"data\"\ndf &lt;- read_csv(file.path(path_data, \"auto_trn.csv\"),\n               col_type = list(mpg = col_factor(levels = c(\"low\", \"high\")),\n                               # here we handle cylinders as an ordered factor\n                               cylinders = col_factor(levels = \n                                                        as.character(c(3,4,5,6,8)), \n                                                      ordered = TRUE),   \n                               displacement = col_double(),\n                               horsepower = col_double(),\n                               weight = col_double(),\n                               acceleration = col_double(),\n                               year = col_double(),\n                               origin = col_factor(levels = \n                                                     c(\"american\", \n                                                       \"japanese\", \n                                                       \"european\")))) %&gt;% \n  glimpse()\n\nRows: 294\nColumns: 8\n$ mpg          &lt;fct&gt; high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    &lt;ord&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement &lt;dbl&gt; 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   &lt;dbl&gt; 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       &lt;dbl&gt; 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration &lt;dbl&gt; 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       &lt;fct&gt; japanese, japanese, european, european, european, europea…\n\n\n\n\n4.3.2 Using a separate mutate()\nIn some instances (e.g., data file with very large number of variables, very consistently organized data character data is well-behaved), you may want to read the data in first and then use mutate() to change classes as needed.\nIn these instances, we prefer to set the col_types() parameter to cols() to prevent the verbose message about column classes.\nHere is an example using the ames dataset with all predictors. To start, we only re-class all character columns to unordered factor and one numeric column to an ordered factor. As we work with the data (during cleaning EDA), we may decide that there are other columns that need to be re-classed. If so, we could add additional lines to the mutuate()\n\ndf &lt;- read_csv(file.path(path_data, \"ames_full_cln.csv\"),\n               col_types = cols()) %&gt;% \n  # convert all character to unordered factors\n  mutate(across(where(is.character), as_factor),\n         overall_qual = ordered(overall_qual, levels = as.character(1:10))) %&gt;% \n  glimpse()\n\nRows: 1,955\nColumns: 81\n$ pid             &lt;fct&gt; x0526301100, x0526350040, x0526351010, x0527105010, x0…\n$ ms_sub_class    &lt;fct&gt; x020, x020, x020, x060, x120, x120, x120, x060, x060, …\n$ ms_zoning       &lt;fct&gt; rl, rh, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl…\n$ lot_frontage    &lt;dbl&gt; 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47, 1…\n$ lot_area        &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10…\n$ street          &lt;fct&gt; pave, pave, pave, pave, pave, pave, pave, pave, pave, …\n$ alley           &lt;fct&gt; none, none, none, none, none, none, none, none, none, …\n$ lot_shape       &lt;fct&gt; ir1, reg, ir1, ir1, reg, ir1, ir1, reg, ir1, ir1, reg,…\n$ land_contour    &lt;fct&gt; lvl, lvl, lvl, lvl, lvl, hls, lvl, lvl, lvl, lvl, lvl,…\n$ utilities       &lt;fct&gt; all_pub, all_pub, all_pub, all_pub, all_pub, all_pub, …\n$ lot_config      &lt;fct&gt; corner, inside, corner, inside, inside, inside, inside…\n$ land_slope      &lt;fct&gt; gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl,…\n$ neighborhood    &lt;fct&gt; n_ames, n_ames, n_ames, gilbert, stone_br, stone_br, s…\n$ condition_1     &lt;fct&gt; norm, feedr, norm, norm, norm, norm, norm, norm, norm,…\n$ condition_2     &lt;fct&gt; norm, norm, norm, norm, norm, norm, norm, norm, norm, …\n$ bldg_type       &lt;fct&gt; one_fam, one_fam, one_fam, one_fam, twhs_ext, twhs_ext…\n$ house_style     &lt;fct&gt; x1story, x1story, x1story, x2story, x1story, x1story, …\n$ overall_qual    &lt;ord&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, …\n$ overall_cond    &lt;dbl&gt; 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6, …\n$ year_built      &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ year_remod_add  &lt;dbl&gt; 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994, …\n$ roof_style      &lt;fct&gt; hip, gable, hip, gable, gable, gable, gable, gable, ga…\n$ roof_matl       &lt;fct&gt; comp_shg, comp_shg, comp_shg, comp_shg, comp_shg, comp…\n$ exterior_1st    &lt;fct&gt; brk_face, vinyl_sd, wd_sdng, vinyl_sd, cemnt_bd, hd_bo…\n$ exterior_2nd    &lt;fct&gt; plywood, vinyl_sd, wd_sdng, vinyl_sd, cment_bd, hd_boa…\n$ mas_vnr_type    &lt;fct&gt; stone, none, brk_face, none, none, none, none, none, n…\n$ mas_vnr_area    &lt;dbl&gt; 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350, 0…\n$ exter_qual      &lt;fct&gt; ta, ta, ta, ta, gd, gd, gd, ta, ta, ta, ta, gd, ex, gd…\n$ exter_cond      &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ foundation      &lt;fct&gt; c_block, c_block, c_block, p_conc, p_conc, p_conc, p_c…\n$ bsmt_qual       &lt;fct&gt; ta, ta, ta, gd, gd, gd, gd, ta, gd, gd, gd, gd, gd, gd…\n$ bsmt_cond       &lt;fct&gt; gd, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ bsmt_exposure   &lt;fct&gt; gd, no, no, no, mn, no, no, no, no, no, gd, av, gd, av…\n$ bsmt_fin_type_1 &lt;fct&gt; blq, rec, alq, glq, glq, alq, glq, unf, unf, unf, glq,…\n$ bsmt_fin_sf_1   &lt;dbl&gt; 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 368,…\n$ bsmt_fin_type_2 &lt;fct&gt; unf, lw_q, unf, unf, unf, unf, unf, unf, unf, unf, unf…\n$ bsmt_fin_sf_2   &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0, 1…\n$ bsmt_unf_sf     &lt;dbl&gt; 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 663…\n$ total_bsmt_sf   &lt;dbl&gt; 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 789,…\n$ heating         &lt;fct&gt; gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a…\n$ heating_qc      &lt;fct&gt; fa, ta, ta, gd, ex, ex, ex, gd, gd, gd, gd, ta, ex, gd…\n$ central_air     &lt;fct&gt; y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ electrical      &lt;fct&gt; s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr…\n$ x1st_flr_sf     &lt;dbl&gt; 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 789…\n$ x2nd_flr_sf     &lt;dbl&gt; 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 672,…\n$ low_qual_fin_sf &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gr_liv_area     &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1…\n$ bsmt_full_bath  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ bsmt_half_bath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ full_bath       &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2, …\n$ half_bath       &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, …\n$ bedroom_abv_gr  &lt;dbl&gt; 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3, …\n$ kitchen_abv_gr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ kitchen_qual    &lt;fct&gt; ta, ta, gd, ta, gd, gd, gd, gd, ta, ta, gd, gd, ex, ta…\n$ tot_rms_abv_grd &lt;dbl&gt; 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, 7,…\n$ functional      &lt;fct&gt; typ, typ, typ, typ, typ, typ, typ, typ, typ, typ, typ,…\n$ fireplaces      &lt;dbl&gt; 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, …\n$ fireplace_qu    &lt;fct&gt; gd, none, none, ta, none, none, ta, ta, ta, gd, po, no…\n$ garage_type     &lt;fct&gt; attchd, attchd, attchd, attchd, attchd, attchd, attchd…\n$ garage_yr_blt   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ garage_finish   &lt;fct&gt; fin, unf, unf, fin, fin, r_fn, r_fn, fin, fin, fin, un…\n$ garage_cars     &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, …\n$ garage_area     &lt;dbl&gt; 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 506,…\n$ garage_qual     &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ garage_cond     &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ paved_drive     &lt;fct&gt; p, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ wood_deck_sf    &lt;dbl&gt; 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, 50…\n$ open_porch_sf   &lt;dbl&gt; 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12, …\n$ enclosed_porch  &lt;dbl&gt; 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ x3ssn_porch     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ screen_porch    &lt;dbl&gt; 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0, 0…\n$ pool_area       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ pool_qc         &lt;fct&gt; none, none, none, none, none, none, none, none, none, …\n$ fence           &lt;fct&gt; none, mn_prv, none, mn_prv, none, none, none, none, no…\n$ misc_feature    &lt;fct&gt; none, none, gar2, none, none, none, none, none, none, …\n$ misc_val        &lt;dbl&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mo_sold         &lt;dbl&gt; 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1, …\n$ yr_sold         &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, …\n$ sale_type       &lt;fct&gt; wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd…\n$ sale_condition  &lt;fct&gt; normal, normal, normal, normal, normal, normal, normal…\n$ sale_price      &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500…\n\n\n\n\n4.3.3 Using data dictionaries (a.k.a codebooks)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>File and Path Management</span>"
    ]
  },
  {
    "objectID": "file_and_path_management.html#sourcing-from-github",
    "href": "file_and_path_management.html#sourcing-from-github",
    "title": "4  File and Path Management",
    "section": "4.4 Sourcing from Github",
    "text": "4.4 Sourcing from Github\nScripts in public repositories on GithHub can be sourced directly from the remote repository on GitHub using source_url() from the `devtools’ package. To do this, follow these steps:\n\nFind the url to the specific file/script you would like to source. This can be done by simply clinical on the file through GitHub in your browser. For example, the url to fun_modeling.R in my lab_support repo is:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R\n\nAdd ?raw=true to the end of that url. For example:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\n\nPass this url as a string into devtools::source_url()` in your R script. For example:\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nIts that easy. Using this method will allow you to continue to use the most up-to-date version of that script even as the repo owner improves it over time. It also doesn’t require you to worry about where a local clone of that repo might live on your computer or the computers of anyone with which you share your code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>File and Path Management</span>"
    ]
  },
  {
    "objectID": "file_and_path_management.html#additional-resources",
    "href": "file_and_path_management.html#additional-resources",
    "title": "4  File and Path Management",
    "section": "4.5 Additional Resources",
    "text": "4.5 Additional Resources\n\nBlog with links on the use of projects and here() package\nGood advice for folder management in projects.\nMore good advice on projects and file management",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>File and Path Management</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html",
    "href": "exploring_dataframes.html",
    "title": "5  Exploring dataframes",
    "section": "",
    "text": "5.1 glimpse()\nglimpse()\nd |&gt; glimpse()\n\nRows: 10\nColumns: 5\n$ x1 &lt;dbl&gt; 9.864656, 10.648627, 9.370226, 8.208932, 7.798212, 10.212464, 11.69…\n$ x2 &lt;dbl&gt; 9.760840, 12.671562, 10.331021, 4.643704, 11.381113, 13.237035, 9.0…\n$ y1 &lt;chr&gt; \"b\", \"o\", \"q\", \"j\", \"e\", \"c\", \"l\", \"j\", \"a\", \"l\"\n$ y2 &lt;chr&gt; \"h\", \"e\", \"a\", \"v\", \"x\", \"v\", \"y\", \"r\", \"v\", \"w\"\n$ z  &lt;fct&gt; fish, cat, dog, fish, fish, fish, cat, cat, cat, fish",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html#glimpse",
    "href": "exploring_dataframes.html#glimpse",
    "title": "5  Exploring dataframes",
    "section": "",
    "text": "Provides info about nrows, ncols, column names, column types and a “glimpse” of some of the data in each column\nreturns the tibble so can be used at the end of a pipe when you first read the data with read_csv() or similar",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html#rows-columns-and-names",
    "href": "exploring_dataframes.html#rows-columns-and-names",
    "title": "5  Exploring dataframes",
    "section": "5.2 Rows, Columns, and Names",
    "text": "5.2 Rows, Columns, and Names\nIf you don’t glimpse the dataframe, you should at least check the number of rows and columns, and review the column names\n\nd |&gt; nrow()\n\n[1] 10\n\nd |&gt; ncol()\n\n[1] 5\n\nd |&gt; names()\n\n[1] \"x1\" \"x2\" \"y1\" \"y2\" \"z\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html#viewing-the-dataframe-directly",
    "href": "exploring_dataframes.html#viewing-the-dataframe-directly",
    "title": "5  Exploring dataframes",
    "section": "5.3 Viewing the dataframe directly",
    "text": "5.3 Viewing the dataframe directly\nI often use view() interactively with the data but view() does not work when rendering quarto documents. You should use head(), tail(), or slice_sample() if you want the output saved in your quarto report.\nhead(), tail() or slice_sample()\n\nhead() and tail() returns the first or last 6 rows of the tibble. Can be changed using n = argumentj\nslice_sample() returns a random row from the tibble. Can be changed to more rows using n = argument\nI find that showing someone some rows with these functions makes the data more real to them. However, works best where there are only a few columns so that they can all be displayed in the console\n\n\nd |&gt; head()\n\n# A tibble: 6 × 5\n     x1    x2 y1    y2    z    \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;\n1  9.86  9.76 b     h     fish \n2 10.6  12.7  o     e     cat  \n3  9.37 10.3  q     a     dog  \n4  8.21  4.64 j     v     fish \n5  7.80 11.4  e     x     fish \n6 10.2  13.2  c     v     fish \n\nd |&gt; tail (n = 3)\n\n# A tibble: 3 × 5\n     x1    x2 y1    y2    z    \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;\n1  8.88 11.0  j     r     cat  \n2  9.12  9.21 a     v     cat  \n3 12.4   9.38 l     w     fish \n\nd |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 5\n     x1    x2 y1    y2    z    \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;\n1 11.7   9.03 l     y     cat  \n2  7.80 11.4  e     x     fish \n3  8.88 11.0  j     r     cat  \n4  9.12  9.21 a     v     cat  \n5 10.6  12.7  o     e     cat",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html#skim",
    "href": "exploring_dataframes.html#skim",
    "title": "5  Exploring dataframes",
    "section": "5.4 skim()",
    "text": "5.4 skim()\nskim()\n\nIncluded in the skimr package\nProvides a detailed summary of the dataframe\nBut the output takes up too much space\nCan use yank() to select only the data types you want to see and you can limit to only some columns if needed.\nCan customize skim to return only the descriptives you want\n\n\n5.4.1 Make your own skimmer\nLets start with a custom skim that returns only the descriptives I generally want\n\nEasiest to start with the base skim()\nThen remove statistics you don’t want by setting to NULL\nThen add any statistics you do want (see example below for syntax)\nDo this for each data type\nHowever, for base (which are reported for all data types), you can’t remove and add, you just need to set what you want\n\n\nlibrary(skimr)\n\nmy_skim &lt;- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),\n                                n_missing = ~sum(is.na(.), na.rm = TRUE)),\n                     numeric = sfl(p25 = NULL,\n                                   p75 = NULL,\n                                   hist = NULL),\n                     character = sfl(min = NULL, max = NULL),\n                     factor = sfl(ordered = NULL))\n\n\n\n5.4.2 Use with all variables\nFirst with all output at once. Does provide summary tables with nrows, ncols, and counts of columns of each datatype. Maybe fine to start (though a bit verbose)\n\nd |&gt; my_skim()\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n10\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\nskim_variable\nn_complete\nn_missing\nempty\nn_unique\nwhitespace\n\n\n\n\ny1\n10\n0\n0\n8\n0\n\n\ny2\n10\n0\n0\n8\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_complete\nn_missing\nn_unique\ntop_counts\n\n\n\n\nz\n10\n0\n3\nfis: 5, cat: 4, dog: 1\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_complete\nn_missing\nmean\nsd\np0\np50\np100\n\n\n\n\nx1\n10\n0\n9.82\n1.47\n7.80\n9.62\n12.44\n\n\nx2\n10\n0\n10.06\n2.39\n4.64\n10.05\n13.24\n\n\n\n\n\n\n\n5.4.3 Use with specific data types\nI prefer to yank() a class/type at a time but then we don’t see rows and columns and all classes present. Could combine with nrow() and ncol()\n\nd |&gt; my_skim() |&gt; \n  yank(\"numeric\")\n\nVariable type: numeric\n\n\n\nskim_variable\nn_complete\nn_missing\nmean\nsd\np0\np50\np100\n\n\n\n\nx1\n10\n0\n9.82\n1.47\n7.80\n9.62\n12.44\n\n\nx2\n10\n0\n10.06\n2.39\n4.64\n10.05\n13.24\n\n\n\n\nd |&gt; my_skim() |&gt; \n yank(\"character\")\n\nVariable type: character\n\n\n\nskim_variable\nn_complete\nn_missing\nempty\nn_unique\nwhitespace\n\n\n\n\ny1\n10\n0\n0\n8\n0\n\n\ny2\n10\n0\n0\n8\n0\n\n\n\n\nd |&gt; my_skim() |&gt; \n  yank(\"factor\")\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_complete\nn_missing\nn_unique\ntop_counts\n\n\n\n\nz\n10\n0\n3\nfis: 5, cat: 4, dog: 1\n\n\n\n\n\n\n\n5.4.4 Limit output to specific columns\nWe can limit the dataframes returned by skimr to a subset of the variables/columns in the original data\n\nThis can be done across or within a data type\nColumns can be selected using tidy select functions\nCan be combined with yank() to limit the output to specific data types if your selected columns are all the same type\n\n\nd |&gt; my_skim(x1, y2) \n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n10\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\nskim_variable\nn_complete\nn_missing\nempty\nn_unique\nwhitespace\n\n\n\n\ny2\n10\n0\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_complete\nn_missing\nmean\nsd\np0\np50\np100\n\n\n\n\nx1\n10\n0\n9.82\n1.47\n7.8\n9.62\n12.44\n\n\n\n\n\n\nd |&gt; my_skim(contains(\"x\")) |&gt; \n  yank(\"numeric\")\n\nVariable type: numeric\n\n\n\nskim_variable\nn_complete\nn_missing\nmean\nsd\np0\np50\np100\n\n\n\n\nx1\n10\n0\n9.82\n1.47\n7.80\n9.62\n12.44\n\n\nx2\n10\n0\n10.06\n2.39\n4.64\n10.05\n13.24",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "exploring_dataframes.html#limit-output-to-specific-descriptive-statistics",
    "href": "exploring_dataframes.html#limit-output-to-specific-descriptive-statistics",
    "title": "5  Exploring dataframes",
    "section": "5.5 Limit output to specific descriptive statistics",
    "text": "5.5 Limit output to specific descriptive statistics\nWe can limit the dataframes returned by skimr to a subset of the statistics using focus()\n\nThis is a variant of dplyr::select() but safer to use with skimmer dataframes\nThis can be done across or within a data type\nMust pre-pend column name with data type (and a .)\nColumns can be selected using tidy select functions\nCan be combined with yank() to limit the output to specific data types\n\n\nd |&gt; my_skim() |&gt;\n  focus(n_missing, numeric.mean) |&gt; \n  yank(\"numeric\")\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\nmean\n\n\n\n\nx1\n0\n9.82\n\n\nx2\n0\n10.06",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html",
    "href": "reproducible_examples.html",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "",
    "text": "6.1 Why create a reprex?\nCareful use of a reproducible example (a reprex) is critical for…\nA reproducible example should be the simplest snippet of code that can reproduce the error that you are encountering. By creating this simple example, it will focus everyone on the nature of your problem(s), reproduce the error message (or incorrect output), and not distract by including code that is irrelevant to your problem.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#why-create-a-reprex",
    "href": "reproducible_examples.html#why-create-a-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "",
    "text": "Debugging your code\nAsking for help on listservs (e.g., Posit/Rstudio community, StackOverflow, and others)\nPosting bug reports for tidymodels or tidyverse packages on GitHub",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "href": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.2 Key Featrures of a Good Reprex",
    "text": "6.2 Key Featrures of a Good Reprex\nThere are a few key features of a good reprex.\n\nThe reprex should run completely on its own with just copy/paste. It should only require the snippet of shared code to run without any additional scripts, data, etc. This makes it easy for everyone to copy your code into their IDE and run it themselves. This is critical if you want someone else to use their time to help you solve YOUR problem.\nIt should NOT require anyone to source other scripts If you used sourced functions, copy them into the reprex. Or better yet, don’t require those functions at all unless they are the source of the error.\nIt should (almost) never require the person to load data. You should either use fake data (i.e., simulated data) that you create at the start of the reprex or data that you load from the modeldata package (i.e., include library(modeldata) at the top of the reprex).\nRemove all extra lines of code from the reprex that are not necessary to either set up the data or produce the error. Your goal is to use the minimum lines of code needed to reproduce the error.\nInclude only necessary packages. Unfortunately, package conflicts are common in R. By stripping out unnecessary packages from your reprex, you may find the source of a package conflict.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#debugging-with-your-reprex",
    "href": "reproducible_examples.html#debugging-with-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.3 Debugging with your Reprex",
    "text": "6.3 Debugging with your Reprex\nYou will find that in the process of simplifying your code, you will find the source of your error yourself in many, many instances. This process of cutting extraneous code and simplifying it is the recommended process for debugging your code systematically.\nOnce you have your simple reprex, you can now experiment with it to find the boundary conditions for the error.\n* Read help on the function that seems to generate the error.\n* Check the name of your variables, the function, and the function parameters.\n* Make sure you are assigning the correct argument to each function parameter. You may want to name the parameters formally rather than relying on the order they are listed in the function).\n* Use R debugging tools like traceback and others.\n* Look carefully at your tibble. Consider the class of all the variables. Are missing values causing an issue?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "href": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.4 Using reprex Package to Post your Reprex",
    "text": "6.4 Using reprex Package to Post your Reprex\nIf you cannot solve the problem after doing substantial testing and debugging within your reprex on your own, it is time to ask for help. In the course, you will ask for help from us (John and the TAs). In the future, you will ask for help from the R community (see options described above). In either instance, be respectful and only ask after you have tried hard to solve the problem on your own and after you’ve truly hit a wall.\nIf you are asking for help, you will need to describe your problem and post your reprex. Try to describe the nature of the issue clearly (though in some instances all that takes is reporting the error message). It might take more description if the problem is not a formal error in the code but incorrect output. The post the reprex. A provides a package (reprex) to help you create markdown code to post your reprex. Here are the steps to use it.\n\nMake sure you have installed the reprex package (install.packages(\"reprex\"); you do not need to load it, only install it).\nCopy our reprex code to your clipboard.\ntype reprex::reprex(venue = \"slack\") (substitute “gh” for slack if you are posting to github, the rstudio/posit community, or StackOverflow).\nAfter the reprex function runs, it will put markdown code onto your clipboard. You can now go back to your post and paste it in. Its that simple!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#using-data-in-your-reprex",
    "href": "reproducible_examples.html#using-data-in-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.5 Using Data in your Reprex",
    "text": "6.5 Using Data in your Reprex\nYou should (almost) never need to provide a separate data file with your reprex (and it just makes things more complicated for those trying to help you if you do).\nInstead, you can often simulate a simple data set. Here is a dataset with variables for different classes including numeric, character and factor.\n\nlibrary(dplyr)\nset.seed(12345) # to make data reproducible if that matters\nd &lt;- tibble (x = rnorm(24), \n             y = rep(c(\"group_1\", \"group_2\"), 12), \n             z = factor(rep(c(\"level_1\", \"level_2\", \"level_3\"), 8))) %&gt;% \n  glimpse()\n\nRows: 24\nColumns: 3\n$ x &lt;dbl&gt; 0.5855288, 0.7094660, -0.1093033, -0.4534972, 0.6058875, -1.8179560,…\n$ y &lt;chr&gt; \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"g…\n$ z &lt;fct&gt; level_1, level_2, level_3, level_1, level_2, level_3, level_1, level…\n\nd %&gt;% print()\n\n# A tibble: 24 × 3\n        x y       z      \n    &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;  \n 1  0.586 group_1 level_1\n 2  0.709 group_2 level_2\n 3 -0.109 group_1 level_3\n 4 -0.453 group_2 level_1\n 5  0.606 group_1 level_2\n 6 -1.82  group_2 level_3\n 7  0.630 group_1 level_1\n 8 -0.276 group_2 level_2\n 9 -0.284 group_1 level_3\n10 -0.919 group_2 level_1\n# ℹ 14 more rows\n\n\nThis should get you started with some edits to fit your scenario. For other situations, the functions caret::twoClassSim() or caret::SLC14_1() might be good tools to simulate data for you.\nAlternatively, you can include library(modeldata) at the top of your reprex and then have access to all the datasets in that package. You can see a list of the dataset names for the repo on Github.\names might be a good starting point for you for fake data because it has lots of options for variables. I find the variable names a bit annoying (because they are not snake_case) but that shouldn’t be an issue for a simple reprex. You should probably select down to just the variables you need and slice out a random sample of fewer cases for use in your reprex. For example…\n\nlibrary(modeldata)\names_1 &lt;- ames %&gt;% \n  select(Sale_Price, Lot_Config, Lot_Frontage) %&gt;% \n  slice_sample(n = 50) %&gt;% \n  glimpse()\n\nRows: 50\nColumns: 3\n$ Sale_Price   &lt;int&gt; 146000, 755000, 115000, 160000, 133000, 187000, 285000, 1…\n$ Lot_Config   &lt;fct&gt; Inside, Corner, Inside, Corner, Inside, Inside, Inside, C…\n$ Lot_Frontage &lt;dbl&gt; 24, 104, 100, 87, 34, 65, 0, 0, 0, 70, 160, 66, 50, 59, 5…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "reproducible_examples.html#more-help",
    "href": "reproducible_examples.html#more-help",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.6 More Help",
    "text": "6.6 More Help\nFor more help see:\n\nHow to use a Reprex from tidyverse folks\n[What is a reprex] (https://github.com/tidyverse/reprex#what-is-a-reprex) from tidyverse folks.\nHow to make a great R reproducible example from Stack Overflow *How to create a reprex from Stack Overflow\nA good example bug report from tidymodels on Github: #46",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How to Create a Reproducible Example (Reprex)</span>"
    ]
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "7  Factors",
    "section": "",
    "text": "7.1 Classing as factor\nx1 &lt;- c(“Dec”, “Apr”, “Jan”, “Mar”) x2 &lt;- c(“Dec”, “Apr”, “Jam”, “Mar”)\nmonth_levels &lt;- c( “Jan”, “Feb”, “Mar”, “Apr”, “May”, “Jun”, “Jul”, “Aug”, “Sep”, “Oct”, “Nov”, “Dec” )\ny1 &lt;- fct(x1, levels = month_levels)\ny2 &lt;- fct(x2, levels = month_levels)\nfct will produce error if value noy in levels when levels supplied\nlevels(y2)\nYou can also create a factor when reading your data with readr with col_factor():\ncsv &lt;- ” month,value Jan,12 Feb,56 Mar,12”\ndf &lt;- read_csv(csv, col_types = cols(month = col_factor(month_levels))) df$month",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#eda",
    "href": "factors.html#eda",
    "title": "7  Factors",
    "section": "7.2 EDA",
    "text": "7.2 EDA\ngss_cat |&gt; count(race)\nbar graph",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#modifying-order",
    "href": "factors.html#modifying-order",
    "title": "7  Factors",
    "section": "7.3 Modifying order",
    "text": "7.3 Modifying order\n\n7.3.1 fct_reorder\nrelig_summary &lt;- gss_cat |&gt; group_by(relig) |&gt; summarize( tvhours = mean(tvhours, na.rm = TRUE), n = n() )\nrelig_summary |&gt; mutate( relig = fct_reorder(relig, tvhours) ) |&gt; ggplot(aes(x = tvhours, y = relig)) + geom_point()\n### fct_relevel\nHowever, it does make sense to pull “Not applicable” to the front with the other special levels. You can use fct_relevel(). It takes a factor, f, and then any number of levels that you want to move to the front of the line.\nggplot(rincome_summary, aes(x = age, y = fct_relevel(rincome, “Not applicable”))) + geom_point()\n\n\n7.3.2 fct_infreq & fct_rev\nFinally, for bar plots, you can use fct_infreq() to order levels in decreasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. Combine it with fct_rev() if you want them in increasing frequency so that in the bar plot largest values are on the right, not the left.\ngss_cat |&gt; mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt; ggplot(aes(x = marital)) + geom_bar()\nfct_inseq(): by numeric value of level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#modifying-factor-levels",
    "href": "factors.html#modifying-factor-levels",
    "title": "7  Factors",
    "section": "7.4 Modifying factor levels",
    "text": "7.4 Modifying factor levels\n\n7.4.1 fct_recode\ngss_cat |&gt; mutate( partyid = fct_recode(partyid, “Republican, strong” = “Strong republican”, “Republican, weak” = “Not str republican”, “Independent, near rep” = “Ind,near rep”, “Independent, near dem” = “Ind,near dem”, “Democrat, weak” = “Not str democrat”, “Democrat, strong” = “Strong democrat” ) )\nfct_recode() will leave the levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist\nTo combine groups, you can assign multiple old levels to the same new level:\ngss_cat |&gt; mutate( partyid = fct_recode(partyid, “Republican, strong” = “Strong republican”, “Republican, weak” = “Not str republican”, “Independent, near rep” = “Ind,near rep”, “Independent, near dem” = “Ind,near dem”, “Democrat, weak” = “Not str democrat”, “Democrat, strong” = “Strong democrat”, “Other” = “No answer”, “Other” = “Don’t know”, “Other” = “Other party” ) )\nIf we want to manipulate a numeric vector, first coerce it to a character, and then recode it. We need to be sure to quote the right half of each of our recoding pairs, since survey’s values are now character (e.g., “1”) rather than numeric (1).\nsurvey &lt;- fct_recode(as.character(survey), “Strongly agree” = “1”, “Agree” = “2”, “Neither agree nor disagree” = “3”, “Disagree” = “4”, “Strongly disagree” = “5”)\n\n\n7.4.2 fct_collapse\nIf you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels:\ngss_cat |&gt; mutate( partyid = fct_collapse(partyid, “other” = c(“No answer”, “Don’t know”, “Other party”), “rep” = c(“Strong republican”, “Not str republican”), “ind” = c(“Ind,near rep”, “Independent”, “Ind,near dem”), “dem” = c(“Not str democrat”, “Strong democrat”) ) )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#setting-factor-contrasts",
    "href": "factors.html#setting-factor-contrasts",
    "title": "7  Factors",
    "section": "7.5 Setting Factor Contrasts",
    "text": "7.5 Setting Factor Contrasts\nNOTE: THIS CAN BE UPDATED TO SIMPLIFY TO JUST SET THE COEFFFICIENTS DIRECTLY. ITS EASIER\n\nsee: https://marissabarlaz.github.io/portfolio/contrastcoding/\nDefault for unordered factors is treatment/dummy\nWe typically want centered orthogonal, and unit weighted.\nWe will use a running example of a factor, \\(x\\), with three levels. We put \\(x\\) in a dataframe to match our typical workflows\nWe then follow with several examples of setting and applying contrasts to \\(x\\)\n\n\n7.5.1 Sample data\n\nlibrary(tidyverse)\nd &lt;- tibble(x = factor(rep(c(\"A\",\"B\", \"C\"), 4)))\nd\n\n# A tibble: 12 × 1\n   x    \n   &lt;fct&gt;\n 1 A    \n 2 B    \n 3 C    \n 4 A    \n 5 B    \n 6 C    \n 7 A    \n 8 B    \n 9 C    \n10 A    \n11 B    \n12 C    \n\n\n\n\n7.5.2 Default contrasts\nThe default contrasts for factors in R as set and viewed using options()\n\nThey are contr.treatment (i.e., dummy codes) for unordered factors\nThey are contr.poly for ordered factors\nWe do not tend to change these defaults but instead explicitly set other contrasts when we need them\n\n\noptions(\"contrasts\")\n\n$contrasts\n        unordered           ordered \n\"contr.treatment\"      \"contr.poly\" \n\n\nWe can confirm the contrasts set by default for \\(x\\) as follows\n\nThey are dummy codes\nThey are named for the level being contrasted with refererence\n\n\ncontrasts(d$x)\n\n  B C\nA 0 0\nB 1 0\nC 0 1\n\n\n\n\n7.5.3 Base R approach for other contrasts\nWe set contrasts using the contrasts() function as well. We could use base R functions that define contrast matrices for classic contrasts as well\n\nBut the coefficients aren’t united weighted (my preference)\nAnd the contrasts aren’t given descriptive labels\n\n\ncontrasts(d$x) &lt;- contr.helmert(levels(d$x))\ncontrasts(d$x)\n\n  [,1] [,2]\nA   -1   -1\nB    1   -1\nC    0    2\n\n\n\n\n7.5.4 An improved approach\nWe tweak this approach to get unit weights (only for Helmert) and meaningful contrast labels\n\nFirst we get the contrast matrix (same as above)\n\n\nc3 &lt;- contr.helmert(levels(d$x))\nc3\n\n  [,1] [,2]\nA   -1   -1\nB    1   -1\nC    0    2\n\n\n\nThen we adjust to make coefficients unit-weighted. This simply requires dividing each column by its range. You could put this in a loop if you had more columns. We generally only do this for helmert or other orthogonal contrasts.\n\n\nc3[, 1] &lt;- c3[, 1] / (max(c3[, 1]) - min(c3[, 1]))\nc3[, 2] &lt;- c3[, 2] / (max(c3[, 2]) - min(c3[, 2]))\n\n\nNow assign names to the columns to label the contrasts\n\n\ncolnames(c3) &lt;- c(\"BvA\", \"CvBA\")\nc3\n\n   BvA       CvBA\nA -0.5 -0.3333333\nB  0.5 -0.3333333\nC  0.0  0.6666667\n\n\n\nAnd now assign these contrasts to \\(x\\)\n\n\ncontrasts(d$x) &lt;- c3\ncontrasts(d$x)\n\n   BvA       CvBA\nA -0.5 -0.3333333\nB  0.5 -0.3333333\nC  0.0  0.6666667\n\n\n\n\n7.5.5 Available contrasts\nThe contrasts we use most regularly are\n\nDummy contrasts (contr.treatment()). These are set by default to unordered factors\nHelmert contrasts (contrl.helmert()).\nEffects constrasts (contr.sum()). We do not generally unit weight these",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "recoding.html",
    "href": "recoding.html",
    "title": "8  Variable recoding, releveling and other transformations",
    "section": "",
    "text": "8.1 if_else()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variable recoding, releveling and other transformations</span>"
    ]
  },
  {
    "objectID": "recoding.html#case_match",
    "href": "recoding.html#case_match",
    "title": "8  Variable recoding, releveling and other transformations",
    "section": "8.2 case_match",
    "text": "8.2 case_match\nreplaces case_when() in IMHO because clearer syntax\n\nd_long &lt;- d_long |&gt; \n  mutate(time_32v1 = case_match(time,\n                                \"time1\" ~ -2/3,\n                                c(\"time2\", \"time3\") ~ 1/3),\n         time_3v2 = case_match(time, \n                               \"time1\" ~ 0,\n                               \"time2\" ~ -.5,\n                               \"time3\" ~ .5))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variable recoding, releveling and other transformations</span>"
    ]
  },
  {
    "objectID": "recoding.html#manipulating-factors",
    "href": "recoding.html#manipulating-factors",
    "title": "8  Variable recoding, releveling and other transformations",
    "section": "8.3 Manipulating factors",
    "text": "8.3 Manipulating factors\n\n8.3.1 Setting contrasts for factor\n\n(helmert2 = matrix(c(-.5, .5), ncol = 1, dimnames = list(c(\"time1\", \"time2\"), c(\"t2v1\"))))\n\n      t2v1\ntime1 -0.5\ntime2  0.5\n\n(helmert3 = matrix(c(-2/3, 1/3, 1/3, 0, -.5, .5), ncol = 2, dimnames = list(c(\"time1\", \"time2\", \"time3\"), c(\"t32v1\", \"t3v2\"))))\n\n           t32v1 t3v2\ntime1 -0.6666667  0.0\ntime2  0.3333333 -0.5\ntime3  0.3333333  0.5\n\n\n\ncontrasts(d2_long$time)\ncontrasts(d2_long$time) &lt;- helmert2\ncontrasts(d2_long$time)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variable recoding, releveling and other transformations</span>"
    ]
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "9  Iteration",
    "section": "",
    "text": "9.1 Introduction and curated resources\nSee this article for details on the use of dplyr for row-wise operations\nSee this article for details on the use of dplyr for column-wise operations\nSee this article for the uses of across() in summarize() and mutate()\nuse list_rbind() with map() to bind together lists of dataframes, which is common when using map()\nIn the sections that follow, we provide common examples of approaches to iteration using the iris dataset.\niris |&gt; glimpse()\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\niris |&gt; head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#select-subset-of-columns",
    "href": "iteration.html#select-subset-of-columns",
    "title": "9  Iteration",
    "section": "9.2 select() subset of columns",
    "text": "9.2 select() subset of columns\nThere is a good tutorial providing more detail on selecting subsets of columns. Here we illustrate a few common applications in our lab.\nIt is easy to select a subset of columns based on their class. Common functions for selecting on column class are * is.numeric * is.factor * is.ordered * is.character.\nThese class functions are used inside of the where() function. For example, here we select all numeric columns. Notice that the parentheses are left off of the is.numeric function when using where()\n\niris |&gt; select(where(is.numeric)) |&gt; \n  glimpse()\n\nRows: 150\nColumns: 4\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n\n\n…and now all factor columns\n\niris |&gt; select(where(is.factor)) |&gt; \n  glimpse()\n\nRows: 150\nColumns: 1\n$ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa…\n\n\nYou can also select columns based on column name. Common helper functions for this include\n\nstarts_with(): Starts with a prefix.\nends_with(): Ends with a suffix.\ncontains(): Contains a literal string.\nmatches(): Matches a regular expression.\nnum_range(): Matches a numerical range like x01, x02, x03.\n\nFor example…\n\niris |&gt; select(contains(\"Width\")) |&gt; \n  glimpse()\n\nRows: 150\nColumns: 2\n$ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4…\n$ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2…",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#get-summary-statistics-for-multiple-columns",
    "href": "iteration.html#get-summary-statistics-for-multiple-columns",
    "title": "9  Iteration",
    "section": "9.3 Get summary statistics for multiple columns",
    "text": "9.3 Get summary statistics for multiple columns\nWe use across() combined with summarize() to get summary statistics across sets of columns. This can also be combined with group_by() to do this within subsets/groups for rows Mean for all numeric columns\n\niris |&gt; \n  summarize(across(where(is.numeric), mean))\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.843333    3.057333        3.758    1.199333\n\n\nMean for width columns grouped by Species\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarize(across(contains(\"Width\"), mean))\n\n# A tibble: 3 × 3\n  Species    Sepal.Width Petal.Width\n  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa            3.43       0.246\n2 versicolor        2.77       1.33 \n3 virginica         2.97       2.03 \n\n\ndf %&gt;% summarise(across(c(col1, col2), list(mean=mean, sd=sd), na.rm=TRUE))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#apply-mutate-to-multiple-columns",
    "href": "iteration.html#apply-mutate-to-multiple-columns",
    "title": "9  Iteration",
    "section": "9.4 Apply mutate() to multiple columns",
    "text": "9.4 Apply mutate() to multiple columns\nWe use across() combined with mutate() to apply the same transformation or other function to multiple columns.\nmultiply values in col1 and col2 by 2 df %&gt;% mutate(across(c(col1, col2), function(x) x*2))\niris %&gt;% mutate(across(c(Sepal.Length, Sepal.Width), round))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#map-and-future_map",
    "href": "iteration.html#map-and-future_map",
    "title": "9  Iteration",
    "section": "9.5 map() and future_map()",
    "text": "9.5 map() and future_map()\ncan generally return lists using map() and then combine into a df afterwards using list_rbind()\nCan use pluck() in a second map if a first map returned a list with multiple elements",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#for-loops",
    "href": "iteration.html#for-loops",
    "title": "9  Iteration",
    "section": "9.6 for loops",
    "text": "9.6 for loops\nDoes someone want o do this?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#foreach-loops",
    "href": "iteration.html#foreach-loops",
    "title": "9  Iteration",
    "section": "9.7 foreach loops",
    "text": "9.7 foreach loops",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#nesting",
    "href": "iteration.html#nesting",
    "title": "9  Iteration",
    "section": "9.8 Nesting",
    "text": "9.8 Nesting\nSome useful tutorials\n\nhttps://r4ds.had.co.nz/many-models.html\nhttps://bookdown.org/Maxine/r4ds/nesting.html\nhttps://tidyr.tidyverse.org/reference/nest.html",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "11  ggplot2",
    "section": "",
    "text": "11.1 Themes\nThis plot shows the default theme for ggplot\nlibrary(tidyverse)\nd  &lt;- tibble(x = rnorm(100), y = rnorm(100))\nd |&gt; ggplot(aes(x = x, y = y)) +\n       geom_point()\nYou can apply other built-in themes by adding them to the plot. Our preferred default is theme_classic()\nd |&gt; ggplot(aes(x = x, y = y)) +\n       geom_point() +\n       theme_classic()\nYou may prefer to increase the size of axis labels, scale, and title (not displayed on this figure). All themes have a base_size parameter which controls the base font size. The base font size is the size that the axis titles use; the plot title is usually bigger (1.2x), and the tick and strip labels are smaller (0.8x). The default base_size = 11.\nHere is an example setting base_size = 14. You might prefer this larger font for figures in a presentation.\nd |&gt; ggplot(aes(x = x, y = y)) +\n       geom_point() +\n       theme_classic(base_size = 16)\nFor consistent figures for all the the figures generated in notebook, you should set the theme at the top of the notebook as part of seting up your environment. This will also remove the need to apply a theme to each figure individually. Use theme_set() for this.\ntheme_set(theme_classic(base_size = 16))\nd |&gt; ggplot(aes(x = x, y = y)) +\n       geom_point()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot2</span>"
    ]
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "12  Simulations",
    "section": "",
    "text": "12.1 The Components of a Simulation",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "simulations.html#the-components-of-a-simulation",
    "href": "simulations.html#the-components-of-a-simulation",
    "title": "12  Simulations",
    "section": "",
    "text": "12.1.1 Setting up Xs (exogenous variables)\nTypically use mvrnorm() in the MASS package to produces a set of \\(X\\) that have some specified population means, variances, and covariances. In our example, we will simulate 3 variables with differing means, variances = 1, and covariances = 0. Of course, you could do whatever you need here.\n\nSpecify population means\n\n\nmeans &lt;- c(4, 6, 10)\n\n\nVariance/covariance matrix. Can do this with uncorrelated Xs with variances of 1 using diag(). Or can specific specific variances and covariances\n\n\nsigma &lt;- diag(3) \n\nsigma\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\nHere we demonstrate use of matrix for specific sigma (but with same values as above)\n\n\nsigma &lt;- matrix(c(1, 0, 0, \n                  0, 1, 0, \n                  0, 0, 1), \n                nrow = 3, byrow = TRUE) \n\nNow we can generate data for a specific \\(N\\) (in this case, 1000)\n\nn_obs &lt;- 1000\nx &lt;- MASS::mvrnorm(n = n_obs, mu = means, Sigma = sigma) |&gt;  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |&gt;  \n    as_tibble() \n\n\nx |&gt; head()\n\n# A tibble: 6 × 3\n     x1    x2    x3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.56  4.94 10.1 \n2  5.47  5.79 10.1 \n3  4.03  5.09  8.73\n4  3.08  4.95  8.87\n5  2.64  5.38  9.15\n6  4.21  4.64 10.6 \n\nx |&gt; summarize(across(everything(), list(mean = mean, var = var)))\n\n# A tibble: 1 × 6\n  x1_mean x1_var x2_mean x2_var x3_mean x3_var\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1    3.97   1.08    6.05  0.990    9.99   1.03\n\ncor(x)\n\n            x1          x2          x3\nx1  1.00000000 -0.02275581  0.01003546\nx2 -0.02275581  1.00000000 -0.03139993\nx3  0.01003546 -0.03139993  1.00000000\n\n\n\n\n12.1.2 Defining a Data Generating Process (DGP) for a Quantitative Y\nWe start with dataframe \\(x\\)\n\nAdd an intercept as first column\n\n\nx &lt;- x |&gt; \n  mutate(x0 = 1) |&gt; \n  relocate(x0)\n\nhead(x)\n\n# A tibble: 6 × 4\n     x0    x1    x2    x3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.56  4.94 10.1 \n2     1  5.47  5.79 10.1 \n3     1  4.03  5.09  8.73\n4     1  3.08  4.95  8.87\n5     1  2.64  5.38  9.15\n6     1  4.21  4.64 10.6 \n\n\n\nSet parameter estimates. This includes \\(b_0\\) as the first entry\n\n\nb &lt;- c(100, 2, 5, 1)\n\n\nSet standard deviation of error\n\n\ne &lt;- 10\n\n\nCalculate Y and put it all in a dataframe\n\n\nd &lt;- x |&gt; \n  mutate(y = rowSums(t(t(x)*b)) + rnorm(n_obs, \n                                        mean = 0, \n                                        sd = e))\n\nhead(d)\n\n# A tibble: 6 × 5\n     x0    x1    x2    x3     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.56  4.94 10.1   151.\n2     1  5.47  5.79 10.1   137.\n3     1  4.03  5.09  8.73  131.\n4     1  3.08  4.95  8.87  133.\n5     1  2.64  5.38  9.15  135.\n6     1  4.21  4.64 10.6   138.\n\n\n\n\n12.1.3 Wrap it all in a function\n\nget_data &lt;- function(s, n_obs, means, sigma, b, e) {\n  \n  x &lt;- MASS::mvrnorm(n = n_obs, mu = means, Sigma = sigma) |&gt;  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |&gt;  \n    as_tibble() |&gt; \n    mutate(x0 = 1) |&gt; \n    relocate(x0)\n  \n  x |&gt; \n    mutate(y = rowSums(t(t(x)*b)) + rnorm(n_obs, 0, e)) |&gt; \n    mutate(s = s) |&gt; \n    relocate(s)\n}\n\nNow we can use this function to simulate a data set with any properties\n\nget_data(1,\n         n_obs &lt;- 1000,\n         means &lt;- c(4, 6, 10), \n         sigma &lt;- diag(3),\n         b &lt;- c(100, 2, 5, 1),\n         e &lt;- 10)\n\n# A tibble: 1,000 × 6\n       s    x0    x1    x2    x3     y\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1  3.73  3.98 10.5   138.\n 2     1     1  4.30  6.66  8.49  152.\n 3     1     1  5.08  6.03  9.95  158.\n 4     1     1  4.32  4.21  6.69  135.\n 5     1     1  3.86  5.59  9.12  143.\n 6     1     1  4.81  6.00 10.6   150.\n 7     1     1  3.26  5.22  9.14  137.\n 8     1     1  3.95  7.07 10.5   136.\n 9     1     1  2.40  7.35 10.9   146.\n10     1     1  3.93  3.76  9.80  140.\n# ℹ 990 more rows\n\n\n\n\n12.1.4 Iterate over simulations\nLets pretend we want 100 simulated datasets\n\nIt is important to set a seed to make these datasets reproducible\nWe can use map (or future_map()) to iterate over get_data()\nWe can use list columns for this so we get a dataframe with rows for each simulated dataset and the full dataset for each entry for the data column\n\n\nn_sims &lt;- 100  # this would generally be much higher!\n\nn_obs &lt;- 1000\nmeans &lt;- c(4, 6, 10)\nsigma &lt;- diag(3)\nb &lt;- c(100, 2, 5, 1)\ne &lt;- 10\n\nsims &lt;- 1:n_sims |&gt; \n  map(\\(s) get_data(s, n_obs, means, sigma, b, e)) |&gt; \n  list_rbind() |&gt; \n  nest(.by = s,\n       .key = \"data\")\n\nhead(sims)\n\n# A tibble: 6 × 2\n      s data                \n  &lt;int&gt; &lt;list&gt;              \n1     1 &lt;tibble [1,000 × 5]&gt;\n2     2 &lt;tibble [1,000 × 5]&gt;\n3     3 &lt;tibble [1,000 × 5]&gt;\n4     4 &lt;tibble [1,000 × 5]&gt;\n5     5 &lt;tibble [1,000 × 5]&gt;\n6     6 &lt;tibble [1,000 × 5]&gt;\n\n\nWe can now use this df with list columns to calculate whatever we are testing for our simulation.\nIn this trivial example, lets show that the parameter estimate for \\(x_1\\) matches what we set. We will also use list columns to store intermedidate results (e.g., the lm)\n\nwrite function to use our simulated data to estimate \\(b_1\\) (Note that we could have used an anonymous function for this, but I think its cleaner to have a named function)\n\n\nfit_lm &lt;- function(d){\n  lm(y ~ x1 + x2 + x3, data = d) \n}\n\n\nNow fit this model for each simulated dataset and save model in list column\nNotice we now have a column that stores these models\n\n\nsims &lt;- sims |&gt; \n  mutate(model = map(data, fit_lm))\n\nhead(sims)\n\n# A tibble: 6 × 3\n      s data                 model \n  &lt;int&gt; &lt;list&gt;               &lt;list&gt;\n1     1 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n2     2 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n3     3 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n4     4 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n5     5 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n6     6 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;  \n\n\nNow lets extract \\(b_1\\) from the models and save in their own column. We can write a function for this and use broom::tidy()\n\nget_b1 &lt;- function(model){\n model |&gt; \n    broom::tidy() |&gt; \n    filter(term == \"x1\") |&gt; \n    pull(estimate)\n}\n\nsims &lt;- sims |&gt; \n  mutate(b1 = map(model, get_b1))\n\nhead(sims)\n\n# A tibble: 6 × 4\n      s data                 model  b1       \n  &lt;int&gt; &lt;list&gt;               &lt;list&gt; &lt;list&gt;   \n1     1 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n2     2 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n3     3 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n4     4 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n5     5 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n6     6 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;   &lt;dbl [1]&gt;\n\n\nNow we can unnest the column for b1 and do analyses. Of course, we could unnest other columns if we needed access to the simuated data or the linear models we fit\n\nsims &lt;- sims |&gt; \n  unnest(b1)\n\nhead(sims)\n\n# A tibble: 6 × 4\n      s data                 model     b1\n  &lt;int&gt; &lt;list&gt;               &lt;list&gt; &lt;dbl&gt;\n1     1 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    1.67\n2     2 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    1.69\n3     3 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    1.91\n4     4 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    2.17\n5     5 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    2.20\n6     6 &lt;tibble [1,000 × 5]&gt; &lt;lm&gt;    2.13\n\n# mean and standard error of sampling distribution\nsims |&gt; \n  summarize(mean = mean(b1), sd = sd(b1))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.01 0.310\n\n# plot of the sampling distribution\nsims |&gt; \n  ggplot(data = sims,\n         mapping = aes(x = b1)) + \n  geom_histogram(bins  = 30)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "simulations.html#other-extensions-and-loose-ends",
    "href": "simulations.html#other-extensions-and-loose-ends",
    "title": "12  Simulations",
    "section": "12.2 Other Extensions and Loose Ends",
    "text": "12.2 Other Extensions and Loose Ends\n\n12.2.1 Binary Y\nIn the example above, we simulated a quantitative Y that was a linear function of 3 Xs. Often, we need to simulate binary outcomes. This is easy too.\n\nWe typically simulate binary outcomes using the logistic function. You could use other functional forms but this one works well for most our needs.\nWe can use our same X\n\n\nhead(x)\n\n# A tibble: 6 × 4\n     x0    x1    x2    x3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.56  4.94 10.1 \n2     1  5.47  5.79 10.1 \n3     1  4.03  5.09  8.73\n4     1  3.08  4.95  8.87\n5     1  2.64  5.38  9.15\n6     1  4.21  4.64 10.6 \n\n\n\nBut now the DGP for Y is different. First we need to calculate the probability of the positive class as a function of X\n\n\ncalc_p &lt;- function(d, b0, b1, b2, b3){\n exp(b0 + b1*x$x1 + b2*x$x2 + b3*x$x3) /\n   (1 + exp(b0 + b1*x$x1 + b2*x$x2 + b3*x$x3))\n}\n\n\nNow we can apply this over the rows of d\n\n\nd &lt;- x |&gt; \n  mutate(p = calc_p(x, 1,2,3,4))\n\nhead(d)\n\n# A tibble: 6 × 5\n     x0    x1    x2    x3     p\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.56  4.94 10.1      1\n2     1  5.47  5.79 10.1      1\n3     1  4.03  5.09  8.73     1\n4     1  3.08  4.95  8.87     1\n5     1  2.64  5.38  9.15     1\n6     1  4.21  4.64 10.6      1\n\n\n\nAnd now we can use the binomial distribution to get binary outcome based on these probabilities\n\n\nd &lt;- d |&gt; \n  mutate(y = rbinom(1000,1,p))\n\nhead(d)\n\n# A tibble: 6 × 6\n     x0    x1    x2    x3     p     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     1  1.56  4.94 10.1      1     1\n2     1  5.47  5.79 10.1      1     1\n3     1  4.03  5.09  8.73     1     1\n4     1  3.08  4.95  8.87     1     1\n5     1  2.64  5.38  9.15     1     1\n6     1  4.21  4.64 10.6      1     1\n\n\n\nAnd of course, we can wrap this all up into one function to simulate these data\n\n\nget_binary_data &lt;- function(s, n_obs, means, sigma, b) {\n  \n  x &lt;- MASS::mvrnorm(n = n_obs, mu = means, Sigma = sigma) |&gt;  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |&gt;  \n    as_tibble() |&gt; \n    mutate(x0 = 1) |&gt; \n    relocate(x0)\n  \n  calc_p &lt;- function(x, b){\n   exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x3) /\n     (1 + exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x3))\n  } \n  \n  x |&gt; \n    mutate(p = calc_p(x, b)) |&gt; \n    mutate(y = rbinom(nrow(x), 1, p))  \n}\n\nFrom here, you can now pick up as before for the quantitative example\n\n\n12.2.2 Fixing the variance of Y to set value\n\ninsert notes from markus\nTalk with markus about need for scaled Y and possible use of mvrnorm() for all variables\n\n\n\n12.2.3 Converting between correlation and covariance matrices\nMASS::mvrnorm() takes sigma (the variance/covariance matrix). Sometimes, we might want to think in terms of correlations.\nHere is a function to convert a correlation matrix to a variance/covariance matrix\n\ncor2cov &lt;- function (r, sd){\n  diag(sd) %*% r %*% diag(sd)\n}\n\n\nr &lt;- matrix(c(1.0, 0.4, 0.3,\n              0.4, 1.0, 0.5,\n              0.3, 0.5, 1.0),\n            nrow = 3, byrow = TRUE)\n\nsd &lt;- c(5, 10, 2)\n\ncor2cov(r, sd)\n\n     [,1] [,2] [,3]\n[1,]   25   20    3\n[2,]   20  100   10\n[3,]    3   10    4",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "geolocation.html",
    "href": "geolocation.html",
    "title": "13  Visualizing geolocation data",
    "section": "",
    "text": "13.1 Loading in and checking data.\nFor these tutorials we will be using data from a single individual collected during the Women’s March on the Washington Mall using the Moves app. These data were pulled from a publicly available online tutorial.\ngps &lt;- read_csv(\"data/moves_example_data.csv\",\n                show_col_types = FALSE) |&gt;\n  janitor::clean_names() |&gt; \n  rename(lon = longitude,\n         lat = latitude) |&gt;\n  glimpse()\n\nRows: 22\nColumns: 7\n$ date     &lt;chr&gt; \"1/21/2017\", \"1/21/2017\", \"1/21/2017\", \"1/21/2017\", \"1/21/201…\n$ name     &lt;chr&gt; \"Place in Penn Quarter, Washington\", \"Place in Federal Triang…\n$ start    &lt;dttm&gt; 2017-01-21 13:49:27, 2017-01-21 14:09:41, 2017-01-21 14:22:1…\n$ end      &lt;dttm&gt; 2017-01-21 13:55:14, 2017-01-21 14:11:41, 2017-01-21 14:29:1…\n$ duration &lt;dbl&gt; 347, 120, 421, 945, 747, 5114, 2459, 483, 1067, 699, 606, 103…\n$ lat      &lt;dbl&gt; 38.89821, 38.89372, 38.89126, 38.88987, 38.89070, 38.88939, 3…\n$ lon      &lt;dbl&gt; -77.02807, -77.02371, -77.01739, -77.01638, -77.01578, -77.01…",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualizing geolocation data</span>"
    ]
  },
  {
    "objectID": "geolocation.html#creating-static-maps",
    "href": "geolocation.html#creating-static-maps",
    "title": "13  Visualizing geolocation data",
    "section": "13.2 Creating static maps",
    "text": "13.2 Creating static maps\nStatic maps can be created using the ggmap library. You will need to set an API key for Stadia Maps to access some of the available designs (specified using the maptype argument in get_map()).\n\nlibrary(ggmap)\nregister_stadiamaps(\"d4bd71d7-556a-4627-9515-ef6e96823ce3\")\n\nHere is an example with points.\n\ncbbox &lt;- make_bbox(lon = gps$lon, lat = gps$lat, f = .1)\nsq_map &lt;- get_map(location = cbbox, maptype = \"stamen_terrain\", source = \"stadia\")\n    \nggmap(sq_map) + \n  geom_point(data = gps, aes(x = gps$lon, y = gps$lat), \n            size = 3, alpha = 0.6, color=\"darkred\") +\n  labs(x = \" \", y = \" \", title = \"Static map (points)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAnd an example with points that vary in size based on duration.\n\ncbbox &lt;- make_bbox(lon = gps$lon, lat = gps$lat, f = .1)\nsq_map &lt;- get_map(location = cbbox, maptype = \"stamen_terrain\", source = \"stadia\")\n    \nggmap(sq_map) + \n  geom_point(data = gps, aes(x = gps$lon, y = gps$lat, size = duration), \n            alpha = 0.6, color=\"darkred\") +\n  labs(x = \" \", y = \" \", title = \"Static map (points based on dur.)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere is an example with connected lines if you were interested in looking at the travel path that an individual took using a geom_path() layer.\n\ncbbox &lt;- make_bbox(lon = gps$lon, lat = gps$lat, f = .1)\nsq_map &lt;- get_map(location = cbbox, maptype = \"stamen_terrain\", source = \"stadia\")\n    \nggmap(sq_map) + \n  geom_path(data = gps, aes(x = gps$lon, y = gps$lat), \n            linewidth = 1, lineend = \"round\", color=\"darkred\") +\n  labs(x = \" \", y = \" \", title = \"Static map (path traveled)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualizing geolocation data</span>"
    ]
  },
  {
    "objectID": "geolocation.html#creating-dynamic-maps",
    "href": "geolocation.html#creating-dynamic-maps",
    "title": "13  Visualizing geolocation data",
    "section": "13.3 Creating dynamic maps",
    "text": "13.3 Creating dynamic maps\nDynamic maps can be helpful visual tools for QC in addition to giving your audience something interesting to look at.\n\n13.3.1 Animated maps\nWe can create animated maps using moveVis, with help from move2 and raster.\n\n# devtools::install_github(\"16EAGLE/moveVis\")\nlibrary(moveVis)\nlibrary(raster, exclude = c(\"select\"))\nlibrary(move2)\nlibrary(magick)\n\nYou will need to first filter out any locations with less than 2 points (there is one such point in our location data). A later function will also be looking for an identification column, which will likely be present for any research participant data (but isn’t in these mock data, so we’ll add it in).\n\ngps_filtered &lt;- gps |&gt; \n  group_by(name) |&gt; \n  filter(n() &gt;= 2) |&gt; \n  ungroup() |&gt; \n  mutate(subid = 1)\n\nNext, we need to convert the GPS data into a move object.\n\ngps_moves &lt;- df2move(gps_filtered,\n        proj = \"EPSG:4326\",  # specifies the coordinate reference system\n        x = \"lon\", y = \"lat\",\n        time = \"start\", # in other datasets, this might be dttm_obs\n        track_id = \"subid\") # your subject identifier goes here\n\nThen, we take our move object and interpolate it to be at regular time intervals.\n\nm &lt;- align_move(gps_moves,\n                res = 5, unit = \"mins\") # data will be aligned to every 5 minutes\n\nTemporal resolution of 5 [mins] is used to align trajectories.\n\n\nNow we can overlay these movement patterns onto a map.\n\nframes &lt;- frames_spatial(m, path_colours = \"darkblue\",\n                         map_service = \"osm_stamen\", map_type = \"terrain\",\n                         map_token = \"d4bd71d7-556a-4627-9515-ef6e96823ce3\", # API key\n                         alpha = 0.5, path_legend = FALSE) |&gt;  \n  add_labels(x = \"Longitude\", y = \"Latitude\") |&gt;\n  add_northarrow() |&gt;  \n  add_scalebar() |&gt; \n  add_timestamps(type = \"label\") |&gt;  \n  add_progress()\n\nChecking temporal alignment...\nProcessing movement data...\nApproximated animation duration: ≈ 4.68s at 25 fps for 117 frames\nRetrieving and compositing basemap imagery...\nLoading basemap 'terrain' from map service 'osm_stamen'...\nAssigning raster maps to frames...\n\n\nIn general, it’s a good idea to look at some frames to make sure the map looks correct. You can index into your frames object using the following code.\n\nframes[[10]]\n\n\n\n\n\n\n\n\nOnce you’ve checked a few frames and it looks like everything is rendering as it should, you can save out the animation as a gif.\n\noutfile &lt;- \"data/moveVis.gif\"\n\nif (!file.exists(outfile)) {\n  animate_frames(frames, out_file = outfile)\n} else {\n  message(\"File already exists, skipping animation rendering.\")\n}\n\nimage_read() from the magick package can be used to display gifs (also works in your Viewer in RStudio!).\n\nimg &lt;- image_read(\"data/moveVis.gif\")\nprint(img)\n\n# A tibble: 117 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 GIF      700    700 sRGB       FALSE        0 72x72  \n 2 GIF      700    700 sRGB       FALSE        0 72x72  \n 3 GIF      700    700 sRGB       FALSE        0 72x72  \n 4 GIF      700    700 sRGB       FALSE        0 72x72  \n 5 GIF      700    700 sRGB       FALSE        0 72x72  \n 6 GIF      700    700 sRGB       FALSE        0 72x72  \n 7 GIF      700    700 sRGB       FALSE        0 72x72  \n 8 GIF      700    700 sRGB       FALSE        0 72x72  \n 9 GIF      700    700 sRGB       FALSE        0 72x72  \n10 GIF      700    700 sRGB       FALSE        0 72x72  \n# ℹ 107 more rows\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Interactive maps",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualizing geolocation data</span>"
    ]
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "14  Programming Notes",
    "section": "",
    "text": "14.1 Advanced data wrangling\nThe tidy folks have also written many more articles on dplyr that are worth a look",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Programming Notes</span>"
    ]
  },
  {
    "objectID": "programming.html#tidy-programming",
    "href": "programming.html#tidy-programming",
    "title": "14  Programming Notes",
    "section": "14.2 Tidy programming",
    "text": "14.2 Tidy programming\nProgramming with dplyr can be complicated because of tidy evaluation. This programming vignette provides useful documentation on data masking and tidy selection\nIf you plan to do a lot of R programming, I highly recommend reading chapters 17-21 on meta-programming in R",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Programming Notes</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "15  Quarto",
    "section": "",
    "text": "15.1 General",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#general",
    "href": "quarto.html#general",
    "title": "15  Quarto",
    "section": "",
    "text": "15.1.1 YAML formats\n\nHTML\nPDF\n\nComprehensive guide to Quarto\n\n\n15.1.2 Chunk options\nChunk options are now specified inside the r backtics, with the following syntax:\n#| echo: false\n#| warning: false\nTo specify these options globally, they need to be added to the _quarto.yml file, with these lines:\nexecute:\n   echo: false\n   warning: false\n\n\n15.1.3 Figures\nTo be included in the quarto figure environment, a figure must have a label that starts with fig-. You can specify the caption as well as the output height/width (in inches) as follows:\n```{r}\n#| label: fig-1\n#| fig-cap: \"A Basic Barplot Figure\"\n#| fig-height: 6\n#| fig-width: 6\n\nggplot(data, aes(x=name, y=value)) + \n  geom_bar(stat = \"identity\")\n```\nTo display a code-generated figure without the caption, the label CANNOT start with fig-.You must also remove the #| fig-cap: option from the chunk, and explicitly include a height and/or width specification, this time using out-width and out-height:\n```{r}\n#| label: a-fig-1\n#| out-width: 6in\n#| out-height: 6in\n\nggplot(data, aes(x=name, y=value)) + \n  geom_bar(stat = \"identity\")\n```\n\n\n15.1.4 Tables\n\n15.1.4.1 HTML\nPlain knitr::kable() renders a nicely striped table in HTML:\n\n\n\n\n\nMake\nModel\nMiles Per Gallon\nCylinders\nGears\n\n\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nDatsun\n710\n22.8\n4\n4\n\n\nHornet\n4\n21.4\n6\n3\n\n\nHornet\nSportabout\n18.7\n8\n3\n\n\n\n\n\nHowever, when you call kableExtra, it does away with the nice striping and spacing and you then have to define that explicitly!\n\n\n\n\n\nMake\nModel\nMiles Per Gallon\nCylinders\nGears\n\n\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nDatsun\n710\n22.8\n4\n4\n\n\nHornet\n4\n21.4\n6\n3\n\n\nHornet\nSportabout\n18.7\n8\n3\n\n\n\n\n\nNote the table above, although the code is identical to the first table, has lost all formatting thanks to invoking kableExtra. The table below re-produces that via kable_styling (with additional features such as floating around text, and footnotes; although it seems the float doesn’t actually work!).\n\n\n\n\n\nMake\nModel\nMiles Per Gallon\nCylinders\nGears\n\n\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nDatsun\n710\n22.8\n4\n4\n\n\nHornet\n4\n21.4\n6\n3\n\n\nHornet\nSportabout\n18.7\n8\n3\n\n\n\nNote: \n\n\n\n\n\n\n Here is a general comments of the table.\n\n\n\n\n\n\n1 Footnote 1;\n\n\n\n\n\n\n2 Footnote 2;\n\n\n\n\n\n\na Footnote A;\n\n\n\n\n\n\nb Footnote B;\n\n\n\n\n\n\n* Footnote Symbol 1;\n\n\n\n\n\n\n† Footnote Symbol 2\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional HTML-only features of kableExtra are documented here\n\n\n15.1.4.2 PDF\nIf using the typst APAish manuscript extension, note that Quarto 1.4XXX uses an older version of Typst without good table support. That should be fixed in the 1.5 release.\nThe documentation for producing LaTeX tables in kableExtra is here\nThe following two tables demonstrate a complex table of the type used in the PDF ema and burden papers (here replicated with a far simpler dataset). This first table is the table code as extracted directly from the manuscript, with no additional formatting. It displays not unlike an HTML table:\n\n\n\nIris\n\n\n\nN\n%\n\n\n\n\nSetosa\n\n\nlarge\nNA\nNA\n\n\nsmall\n50\n0.3\n\n\nVersicolor\n\n\nlarge\n43\n0.3\n\n\nsmall\n7\n0.0\n\n\nVirginica\n\n\nlarge\n50\n0.3\n\n\nsmall\nNA\nNA\n\n\n\nNote: \n\n\n\n\n N = 75\n\n\n\n\n\n\n\n\n\n\nIn LaTeX, this table displays quite differently. Below, the same code has been modified to reproduce exactly the look of the table as it appears in the PDF manuscript. That is, the code in the chunk above produces the output below when rendered with knitr to PDF; but rendering that output to HTML requires the additions in the chunk below:\n\nbootstrap_options = \"none\" added to kable_styling removes the default horizontal lines, under each row, but then font size and table width must now be specified.\npack_rows() adds horizontal lines in HTML but not LaTeX; those must be removed with the label_row_css option .\nFinally, column width is specified, and horizontal lines around the column headers and before the footnote, are added with row_spec()’s extra_css option.\n\n\n\n\nIris\n\n\n\nN\n%\n\n\n\n\nSetosa\n\n\nlarge\n\n\n\n\nsmall\n50\n0.3\n\n\nVersicolor\n\n\nlarge\n43\n0.3\n\n\nsmall\n7\n0.0\n\n\nVirginica\n\n\nlarge\n50\n0.3\n\n\nsmall\n\n\n\n\n\nNote: \n\n\n\n\n N = 75\n\n\n\n\n\n\n\n\n\n\n\n\n15.1.4.3 Useful Table Options and features\nTo be sure tables display with NA cells as blanks (instead of “NA”) include this before your first table:\noptions(knitr.kable.NA = \"\")\nkableExtra has a function is collapse_rows, yet that seems to be persistently broken\nrow_spec() has a parameter, hline_after, which, ostensibly, should create a horizontal line under that row, right? Apparently it only works on LaTeX although this is undocumented. Apparently the solution is to add row_spec(X, extra_css = \"border-bottom: 1px solid\")) (where x is the rownum; 0 for the header, nrow(df) for the last row)\nSimilarly, to supress hlines on headers created by pack_rows(), you need to add extra_css = \"border-bottom: none\" into the pack_rows() command.\n\n\n\n15.1.5 Rendering\nFor rendering books and slides, we have written a bash script to handle setting the _quarto.yml file and then rendering the full project or a single file.\n\ncall the function from the terminal: ./render.sh filename.qmd book\nthe first parameter can be the name of the qmd file or all to render all units (only used for books, not slides)\nthe second parameter can be book, slides or slides_wide\n\n\n15.1.5.1 Full book/website rendering\nFor books and websites, which have many .qmd documents linked together, you would render the full book by calling quarto::quarto_render() to render all documents in the active project directory.\nFor books and websites, if anything changes in the .yml file (such as addition or deletion of a chapter), you MUST re-render the entire directory in this manner, otherwise the added/deleted chapter will not be rendered correctly.\n\n\n15.1.5.2 Rendering other formats and targets\nYou can specify render targets and document render ordering more specifically in project metadata, see https://quarto.org/docs/projects/quarto-projects.html#render-targets\n\n\n15.1.5.3 PDF\nYou can specify rendering to PDF format by adding the following to your .yml file:\nformat: pdf: documentclass: book\nYou can then explictly render just the PDF format by specifying both the output format and the filename in the call to quarto_render:\nquarto::quarto_render(output_format = “pdf”,output_file = “dwt.pdf”)\n\n15.1.5.3.1 PDF Fonts\nTo specify a font you add the following to the YAML:\nIf using Xelatex:\nmainfont : FontName # for document text\nsansfont : FontName # for H1-H6 text\nmonofont : FontName  # for code chunk text\nIf using pdflatex:\nfontfamily : FontName # for document text\nWhere FontName is the name of your desired system font. To find available system fonts, run, systemfonts::system_fonts() and look for the name of the font (not the name of the font .tff file). For example, if you wish to use Arial font, your FontName should be “ArialMT”, rather than “arial” or “Arial”.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#documents",
    "href": "quarto.html#documents",
    "title": "15  Quarto",
    "section": "15.2 Documents",
    "text": "15.2 Documents\nWe use Quarto documents for two purposes - reproducible analyses and submitted papers. We will generally render analysis doc to html and papers to pdf. The Quarto Guide provides more detail on on creating pdf and html\nThese documents are styled primarily using Markdown. The Quarto Guide provides more detail on markdown basics",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#journal-articles",
    "href": "quarto.html#journal-articles",
    "title": "15  Quarto",
    "section": "15.3 Journal Articles",
    "text": "15.3 Journal Articles\n\n15.3.1 Templates\nQuarto does have a system for applying journal article templates to quarto docs, see this reference page\nThe list of available templates is still somewhat small, but extending them seems doable.\n\nInfo on creating a custom template\nTemplate for creating a custom template\n\nRelated reference links for creating an AJP template:\n\nAJP Manuscript requirements\n\nExample AJP Latex Template\nMore APA style templates\n\nTypst APAish manuscript\n\n\n\n15.3.2 Bibliographies\nTo add a bibliography with a citation style, add the following lines to the _quarto.yml or the .qmd file:\nbibliography: references.bib\ncsl: name_of_csl_file.csl\nFor a typst APAish manuscript your yaml will look like this to use the built-in typst citation style:\nbibliography: references.bib\nbibliographystyle: apa\nOr download a local copy of the desired .csl file:\nbibliography: references.bib\nbibliographystyle: name_of_csl_file.csl\nAt the moment, this extension doesn’t support linking to the CSL repo (below).\nNote, we will typically call our csl files from the official Citation Style Language Repo; which simply requires the URL of the raw file from github.\nSee docs_arc for specifics about commonly-used ARC csl files.\nThe Quarto Guide provides more detail about how to work with citations and footnotes. As with markdown, the format is [@citekey1; @citekey2] — citations go inside square brackets and are separated by semicolons",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#reproducible-analyses",
    "href": "quarto.html#reproducible-analyses",
    "title": "15  Quarto",
    "section": "15.4 Reproducible Analyses",
    "text": "15.4 Reproducible Analyses",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#presentations",
    "href": "quarto.html#presentations",
    "title": "15  Quarto",
    "section": "15.5 Presentations",
    "text": "15.5 Presentations\nWe use Quarto to make revealjs slide decks for presentations. The Quarto Guide provides extensive documentation and sample slides. You can begin with the overview of presentations across formats (Quarto can also render powerpoint and other formats). Follow this with a revealjs overview and then the revealjs reference chapter. The Quarto Guide also provides a chapter on presenter tools.\nDivs (:::) and spans([]) are used extensively in presentations. An introduction to their use is provided in the markdown basics chapter. The Pandoc manual provides more detail.\nIn a project, presentations should be located in subfolder that starts with an underscore, ie. _presentations, to prevent those files from being rendered during the project render.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#books-and-websites",
    "href": "quarto.html#books-and-websites",
    "title": "15  Quarto",
    "section": "15.6 Books and Websites",
    "text": "15.6 Books and Websites\nInformation on setting up a book: https://quarto.org/docs/books/\nInformation on setting up a website: https://quarto.org/docs/websites/\nInformation on setting up Github Pages to publish a book/website on commit: https://quarto.org/docs/publishing/github-pages.html#render-to-docs\nThe difference between a book and a website in quarto is the syntax in the _quarto.yml file. Books have chapters, websites have sections; there are also a few differences in the types of metadata options you can specify (for example, books have titles and authors, websites do not). The main visible difference in the published output is that book chapters are numbered, while website sections are not.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#publishing",
    "href": "quarto.html#publishing",
    "title": "15  Quarto",
    "section": "15.7 Publishing",
    "text": "15.7 Publishing\nYou can publish documents, books, and presentations to a variety of places.\n\n15.7.1 Presentations and Documents\nOur preferred location for presentations (and Quarto Docs) is Quarto Pub. This site is public and free. To use it, you need to set up an account first.\nTo publish a presentation (or other Quarto doc) to Quarto Pub, you should first log in on your default browsers. You should next go to the Terminal tab in the RStudio IDE. Navigate to the folder that contains your presentation. Then type quarto publish quarto-pub. If this is the first time you are publishing at Quarto Pub on that computer, you will need to authorize it. Follow the prompts in the web browser and then in the terminal to complete the publication process. This authorization is saved in a file called _publish.yml, which will be accessed for future updates to the presentation.\nSee additional instructions in the Quarto guide if necessary.\n\n\n15.7.2 Books and Websites\nWe have chosen to use Github Pages to publish several of our books. See this link for detailed information on setting up your repo to have a Github Page and to use Github Actions to publish.\nBasically, any commit which includes changed html files tells Github Actions to re-build the book. The publishing workflow therefore consists of a) rendering locally, and b) committing the newly-rendered documents via github desktop. It usually takes &lt;2 minutes for Github Actions to build and deploy the updated pages",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#terminal-commands",
    "href": "quarto.html#terminal-commands",
    "title": "15  Quarto",
    "section": "15.8 Terminal Commands",
    "text": "15.8 Terminal Commands\n\nquarto publish quarto-pub to publish a presentation or document to Quarto Pub\nquarto render filename.qmd --to pdf\nquarto render filename.qmd --to html\nquarto render docname.qmd -o outputname.html\nquarto render docname.qmd -P pv1:1 -P pv2:5\n/opt/quarto/\"${QUARTO_VERSION}\"/bin/quarto check to check installation details",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#latex-and-quarto",
    "href": "quarto.html#latex-and-quarto",
    "title": "15  Quarto",
    "section": "15.9 Latex and Quarto",
    "text": "15.9 Latex and Quarto\nFormatting of a PDF rendered from Quarto can be finely controlled with the addition of LaTeX commands. See this Quarto reference as well as some examples below:\n\n15.9.1 Inline Commands\nCertain latex commands can be placed in qmd files as plaintext (on their own line, with surrounding blank lines), for example: \\newpage starts a new page, \\hline adds a horizontal line.\n\n\n15.9.2 Chunk Commands\nMultiline commands can be wrapped in latex code chunks:\n\n\n15.9.3 YAML\n\\raggedright added to the include-in-header YAML command, ensures lines are flush with the left margin and ragged on the right margin (vs justified where text is stretched to ensure both margins are flush)\nWe wuse the following in the YAML header to set up a html document to render as a single file with a TOC.\n\nformat: \n  html: \n    embed-resources: true\n    toc: true \n    toc_depth: 4\n\n\n\n15.9.4 Packages\n\\usepackage{wrapfig} - allow text wrapping \\usepackage{float} - allow use of float options such as H (use UPPERCASE to preserve floats) \\usepackage{caption} - allows supression of automatic caption numbering \\usepackage{lscape} - allows insertion of a landscape oriented page \\usepackage{enumitem} - allows fine-grain control over list styling\n\n\n15.9.5 Re-render and freeze\nhttps://quarto.org/docs/projects/code-execution.html",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#using-parameters",
    "href": "quarto.html#using-parameters",
    "title": "15  Quarto",
    "section": "15.10 Using Parameters",
    "text": "15.10 Using Parameters\nYou define parameters in the YAML using the syntax below\n\nYou can assign initial values to the parameters. These values will be used by default if you do not replace them with new values from the command line when you render the script.\nYou can update these values with new values you pass in when you render the script (see below).\n\nIf you only provide a subset of updated parameter values when you render the document, the default values will be used for the remaining parameters.\n\n\nparams:\n  pv1: 5\n  pv2: 10\n\nIn some instances, I prefer to assign the values from the params list to individual variables\n\nCode is shorter when using these variables\nI can update them interactively if I want to use different values (the params list is read-only)\n\nFirst set the parameters in the YAML as above. Then put this code chunk right at the top of the script. If you edit this code chunk to assign new values interactively, make sure you comment it out when you save the final script!\n\npv1 &lt;- params$pv1\npv2 &lt;- params$pv2\n# pv1 &lt;- 5 # Use this line to interactively assign new value\n# pv2 &lt;- 10 # use this line to interactively assign new value\n\n\n15.10.1 Using parameter values\nWhen you use params in the YAML, a list named params is created.\n\nYou can then use this list as normal with no further code needed to establish the values.\n\n\nparams$pv1 + params$pv2\n\n\nYou can also access these values using inline r statements. See example below. I strongly recommend using this in the title of your document so that you can confirm that you correctly updated the parameter values when you rendered! e.g.,\n\n\ntitle: \"Demo of quarto document with pv1 = `r params$pv1` and  pv2 = `r params$pv2`.\"\n\n\nOr if you assigned the parameter values to variables as I recommended, you can just use those variables as you normally would. They will start with the values assigned to associated parameters (in YAML or input from command line)\n\n\n\n15.10.2 Passing parameter values at command line\nI prefer to render quarto documents in the terminal.\n\nYou can indicate the output filename (otherwise, the output file name is set to the input filename with a different extension).\n\nYou can also pass in values to the parameters, which is typically why we use parameters in the first place\n\nUse the following syntax to render quarto documents at command line.\n\nuse -P to provide a parameter value. No space between parameter name and value. If you provide values for only a subset of parameters, defaults will be used for other parameters\nuse -o to specify filename (defaults to input filename if not provided).\n\n\nquarto render docname.qmd -o outputname.html  -P pv1:1 -P pv2:5",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#manuscript-projects",
    "href": "quarto.html#manuscript-projects",
    "title": "15  Quarto",
    "section": "15.11 Manuscript Projects",
    "text": "15.11 Manuscript Projects\nAs of 2024 our preferred method for creating reproducible analysis is to have a quarto project for each study.\nA project allows you to connect a website, presentations (both slidesets and posters), and PDFs/word documents to a set of analysis notebooks (formerly our ana_.Rmd files). That way, code across all output formats can be updated simultaneously from a single location. Plots and charts can also be produced from these notebooks, which can them be used across multiple output document formats.\nPlease note that most of the documentation on John’s Data Wrangling quarto page was written when we were creating each format type as a standalone. This information is still valid for that workflow. Differences related to projects will be noted in the appropriate section on that page.\nHere are steps to set up a manuscript project:\n\nCreate study repo for project on github in John’s account (call it study_NAME)\nAdd collaborators to the repo\nClone the repo to your computer\nCreate manuscript project files in the repo. In the terminal (in github folder above new repo) type quarto create project manuscript and then indicate the name of the repo folder when asked\nAdd a gh-pages branch to the repo. Do this in terminal with these commands\n\n\ngit switch --orphan gh-pages\ngit commit --allow-empty -m \"Initial commit on orphan branch\"\ngit push -u origin gh-pages\n\n\nSet up github to publish website from the gh-pages branch. Go to settings, pages, and select the gh-pages and /root as the branch to publish from. The website will be published at https://jjcurtin.github.io/study_NAME",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-templates",
    "href": "quarto.html#quarto-templates",
    "title": "15  Quarto",
    "section": "15.12 Quarto Templates",
    "text": "15.12 Quarto Templates\nAll of the following templates are quarto files for use in creating lab-related documentation\n\nSimple Quarto Template. This simple template allows you to set margins/font and to switch from HTML to PDF output.\n\n\n15.12.1 YAML files\nIncluding these YAML files in a blank .qmd document sets up output for either HTML or PDF, tailored to ARC standards with respect to margins, font, etc.\nTBD (CONTENTS NEED TO BE ADDED TO THESE FILES)\n\nHTML YAML\nPD YAML\n\n\n\n15.12.2 Manuscripts\n\n15.12.2.1 APA\n.qmd template. This is based on the template at this repo. Our version is currently under development to try and load assets from our lab_support repo rather than installing the extention files locally.\n\n\n15.12.2.2 CSL files\nBelow are the YAML code of a couple that we use frequently (linked to the primary CSL repo)\nNIH Grant Proposals:\ncsl: https://raw.githubusercontent.com/citation-style-language/styles/master/national-library-of-medicine-grant-proposals.csl\nElsevier (Vancouver substyle):\ncsl: https://raw.githubusercontent.com/citation-style-language/styles-distribution/f8524f9b9df60e94e98f824f242a1fb27cc9fc59/elsevier-vancouver.csl\nAPA 7th Edition:\ncsl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\nNote: the apa quarto template in the lab_support repo calls its own version of the apa.csl from within its ex_apa support folder.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#letterhead",
    "href": "quarto.html#letterhead",
    "title": "15  Quarto",
    "section": "15.13 letterhead",
    "text": "15.13 letterhead\nLetterhead: .qmd template. See Susan or John for assistance in personalizing this for other lab members.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "datetimes_dates.html",
    "href": "datetimes_dates.html",
    "title": "17  Exploring date-times and dates with lubridate",
    "section": "",
    "text": "17.1 Date-times/dttm\nd &lt;- tibble(t = c(\"2017-01-27 12:01:00\", \"2017-01-27 23:59:00\", \"2017-01-27 00:01:00\")) %&gt;% \n  glimpse()\n\nRows: 3\nColumns: 1\n$ t &lt;chr&gt; \"2017-01-27 12:01:00\", \"2017-01-27 23:59:00\", \"2017-01-27 00:01:00\"\nCan use as_datetime to class as dttm\nd %&gt;% \n  mutate(t_dttm = as_datetime(t)) %&gt;% \n  print() %&gt;% \n  pull(t_dttm) %&gt;% \n  print()  \n\n# A tibble: 3 × 2\n  t                   t_dttm             \n  &lt;chr&gt;               &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 12:01:00\n2 2017-01-27 23:59:00 2017-01-27 23:59:00\n3 2017-01-27 00:01:00 2017-01-27 00:01:00\n[1] \"2017-01-27 12:01:00 UTC\" \"2017-01-27 23:59:00 UTC\"\n[3] \"2017-01-27 00:01:00 UTC\"\nd %&gt;% \n  mutate(t_dttm = as_datetime(t, tz= \"America/Chicago\")) %&gt;% \n  print() %&gt;% \n  pull(t_dttm) %&gt;% \n  print()  \n\n# A tibble: 3 × 2\n  t                   t_dttm             \n  &lt;chr&gt;               &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 12:01:00\n2 2017-01-27 23:59:00 2017-01-27 23:59:00\n3 2017-01-27 00:01:00 2017-01-27 00:01:00\n[1] \"2017-01-27 12:01:00 CST\" \"2017-01-27 23:59:00 CST\"\n[3] \"2017-01-27 00:01:00 CST\"\nd %&gt;% \n  mutate(t_dttm = as_datetime(t, tz= \"America/Chicago\"),\n         t_dttm = with_tz(t_dttm, tz = \"UTC\")) %&gt;% \n  print() %&gt;% \n  pull(t_dttm) %&gt;% \n  print()  \n\n# A tibble: 3 × 2\n  t                   t_dttm             \n  &lt;chr&gt;               &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 18:01:00\n2 2017-01-27 23:59:00 2017-01-28 05:59:00\n3 2017-01-27 00:01:00 2017-01-27 06:01:00\n[1] \"2017-01-27 18:01:00 UTC\" \"2017-01-28 05:59:00 UTC\"\n[3] \"2017-01-27 06:01:00 UTC\"\nd %&gt;% \n  mutate(t_dttm = as_datetime(t, tz= \"America/Chicago\"),\n         t_dttm_forced = force_tz(t_dttm, tzone = \"UTC\")) %&gt;% \n  print() %&gt;% \n  pull(t_dttm_forced) %&gt;% \n  print()  \n\n# A tibble: 3 × 3\n  t                   t_dttm              t_dttm_forced      \n  &lt;chr&gt;               &lt;dttm&gt;              &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 12:01:00 2017-01-27 12:01:00\n2 2017-01-27 23:59:00 2017-01-27 23:59:00 2017-01-27 23:59:00\n3 2017-01-27 00:01:00 2017-01-27 00:01:00 2017-01-27 00:01:00\n[1] \"2017-01-27 12:01:00 UTC\" \"2017-01-27 23:59:00 UTC\"\n[3] \"2017-01-27 00:01:00 UTC\"\nd %&gt;% \n  mutate(t_dttm = as_datetime(t, tz= \"America/Chicago\")) %&gt;% \n  print() %&gt;% \n  write_csv(file.path(path, \"dttm_output.csv\")) %&gt;% \n  print()\n\n# A tibble: 3 × 2\n  t                   t_dttm             \n  &lt;chr&gt;               &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 12:01:00\n2 2017-01-27 23:59:00 2017-01-27 23:59:00\n3 2017-01-27 00:01:00 2017-01-27 00:01:00\n# A tibble: 3 × 2\n  t                   t_dttm             \n  &lt;chr&gt;               &lt;dttm&gt;             \n1 2017-01-27 12:01:00 2017-01-27 12:01:00\n2 2017-01-27 23:59:00 2017-01-27 23:59:00\n3 2017-01-27 00:01:00 2017-01-27 00:01:00\nd_csv &lt;- read_csv(file.path(path, \"dttm_output.csv\")) %&gt;% \n  glimpse()\n\nRows: 3\nColumns: 2\n$ t      &lt;dttm&gt; 2017-01-27 12:01:00, 2017-01-27 23:59:00, 2017-01-27 00:01:00\n$ t_dttm &lt;dttm&gt; 2017-01-27 18:01:00, 2017-01-28 05:59:00, 2017-01-27 06:01:00\n\nd_csv$t   # CAREFUL if t wasn't in UTC before, this has changed its moment in time when setting to UTC.\n\n[1] \"2017-01-27 12:01:00 UTC\" \"2017-01-27 23:59:00 UTC\"\n[3] \"2017-01-27 00:01:00 UTC\"\n\nd_csv$t_dttm\n\n[1] \"2017-01-27 18:01:00 UTC\" \"2017-01-28 05:59:00 UTC\"\n[3] \"2017-01-27 06:01:00 UTC\"",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exploring date-times and dates with lubridate</span>"
    ]
  },
  {
    "objectID": "datetimes_dates.html#date-timesdttm",
    "href": "datetimes_dates.html#date-timesdttm",
    "title": "17  Exploring date-times and dates with lubridate",
    "section": "",
    "text": "Create a tibble with a date as a character string\n\n\n\n\ndefaults to assuming character string is in UTC tz\nwill class as dttm in UTC\n\n\n\nCan class character string as dttm in another timezone with \\(tz\\) argument\nWill class as dttm in that other timezone retaining original values but just with different timezone\n\n\n\nCan class character string as dttm in another timezone with \\(tz\\)\nAND THEN change timezone (e.g., orginal values were in America/Chicago and then change to UTC tz)\n\\(with\\_tz()\\) returns same moment of time in another timezone\n\n\n\nCan force a shift of the time zone keeping the actual time value the same\nThis is a NEW moment in time.\n\nDo if the dttm already has a tz and it is WRONG\nSee \\(force_tzs()\\) if you have dttm in same column which need to be shifted to varied tzs\n\nAll values in a column in R need to have same tz\n\\(force\\_tzs()\\) can accomodate this b/c forcing the tz changes and then returning all values in one time zone (with that latter adjustement preserving their new moments in time)\n\n\n\n\nWhen using write_csv() and vroom_write(), dttm is converted to UTC first\n\nA Z is appended to time to indicate (Z)ero time shift\nChange back to appropriate timezone using with_tz() in a mutate()\n\nThe character string t is written as a character string\n\nIt was not a dttm so it is not converted to UTC\nIt has no Z\nHOWEVER, when you read this back, it will assume UTC!!!!\nYou will need to use \\(force\\_tz()\\) if it was actually in another tz\nOpen dttm_output.csv to see this behavior\n\n\n\n\nWhen reading dttm_output.csv, t_dttm is clearly UTC with Z suffix\nt is clearly an R formatted date-time it is classed as dttm\n\nIt had no tz info so it is assumed to be in UTC!\nShift it to get back to whatever tz you want/need usig \\(force\\_tz()\\)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exploring date-times and dates with lubridate</span>"
    ]
  },
  {
    "objectID": "datetimes_dates.html#dates",
    "href": "datetimes_dates.html#dates",
    "title": "17  Exploring date-times and dates with lubridate",
    "section": "17.2 Dates",
    "text": "17.2 Dates\n\nUse \\(as\\_date()\\) to convert character string to date\ndate class does NOT have tz\n\n\n(a_date = as_date(\"2017-01-27\"))\n\n[1] \"2017-01-27\"\n\nclass(a_date)\n\n[1] \"Date\"\n\n\n\nCan strip time from dttm with \\(as\\_date()\\)\n$as_date() ignores tz and just parse out the date portion of the dttm\n\n\n(a_dttm = as_datetime(\"2017-01-27 23:59:01\", tz = \"America/Chicago\"))\n\n[1] \"2017-01-27 23:59:01 CST\"\n\nas_date(a_dttm)\n\n[1] \"2017-01-27\"\n\n(a_dttm = as_datetime(\"2017-01-27 23:59:01\", tz = \"UTC\"))\n\n[1] \"2017-01-27 23:59:01 UTC\"\n\nas_date(a_dttm)\n\n[1] \"2017-01-27\"\n\n\n\nNote that base R as.Date() works differently with respect to how it handles the tz of the dttm\nWe will always use \\(as\\_date()\\)\n\n\n(a_dttm = as_datetime(\"2017-01-27 23:59:01\", tz = \"America/Chicago\"))\n\n[1] \"2017-01-27 23:59:01 CST\"\n\nas.Date(a_dttm)\n\n[1] \"2017-01-28\"\n\n(a_dttm = as_datetime(\"2017-01-27 23:59:01\", tz = \"UTC\"))\n\n[1] \"2017-01-27 23:59:01 UTC\"\n\nas.Date(a_dttm)\n\n[1] \"2017-01-27\"\n\n\n\nWhen writing to csv, outputs as character. Since no tz, not tz stamp is needed\nWhen reading, the character string is automatically converted to date class\n\n\nd &lt;- tibble(dates = c(\"2017-01-27\", \"2017-01-28\",\"2017-01-29\")) %&gt;% \n  mutate(dates_asdates = as_date(dates)) %&gt;% \n  glimpse() %&gt;% \n  print()\n\nRows: 3\nColumns: 2\n$ dates         &lt;chr&gt; \"2017-01-27\", \"2017-01-28\", \"2017-01-29\"\n$ dates_asdates &lt;date&gt; 2017-01-27, 2017-01-28, 2017-01-29\n# A tibble: 3 × 2\n  dates      dates_asdates\n  &lt;chr&gt;      &lt;date&gt;       \n1 2017-01-27 2017-01-27   \n2 2017-01-28 2017-01-28   \n3 2017-01-29 2017-01-29   \n\nwrite_csv(d, file.path(path, \"date_output.csv\"))\n\nd_csv &lt;- read_csv(file.path(path, \"date_output.csv\")) %&gt;% \n  glimpse() %&gt;% \n  print()\n\nRows: 3\nColumns: 2\n$ dates         &lt;date&gt; 2017-01-27, 2017-01-28, 2017-01-29\n$ dates_asdates &lt;date&gt; 2017-01-27, 2017-01-28, 2017-01-29\n# A tibble: 3 × 2\n  dates      dates_asdates\n  &lt;date&gt;     &lt;date&gt;       \n1 2017-01-27 2017-01-27   \n2 2017-01-28 2017-01-28   \n3 2017-01-29 2017-01-29",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exploring date-times and dates with lubridate</span>"
    ]
  },
  {
    "objectID": "datetimes_dates.html#converting-unix-times",
    "href": "datetimes_dates.html#converting-unix-times",
    "title": "17  Exploring date-times and dates with lubridate",
    "section": "17.3 Converting unix times",
    "text": "17.3 Converting unix times\n\n# https://www.epochconverter.com/\n# Epoch timestamp: 1485540001\n# Timestamp in milliseconds: 1485540001000\n# Human time (GMT): Friday, January 27, 2017 6:00:01 PM\n# Human time (your time zone): Friday, January 27, 2017 12:00:01 PM GMT-06:00\n\nas_datetime(1485540001)  #assumes origin is 1970-01-01 unix time and returns UTC timezone\n\n[1] \"2017-01-27 18:00:01 UTC\"\n\nas_datetime(1485540001, tz='America/Chicago') # same moment in time in different time zone\n\n[1] \"2017-01-27 12:00:01 CST\"",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exploring date-times and dates with lubridate</span>"
    ]
  },
  {
    "objectID": "datetimes_dates.html#conclusions",
    "href": "datetimes_dates.html#conclusions",
    "title": "17  Exploring date-times and dates with lubridate",
    "section": "17.4 Conclusions",
    "text": "17.4 Conclusions\nLab practices:\n\nOnly use lubridate functions for manipulation of date-time and date objects\nRecognize that dttm columns saved/read from csv in UTC with Z timezone stamp\nQualtrics doesn’t timestamp its time variables. We will set up qualtrics to always export in UTC\nWe can change timezones in our scripts as needed (or leave in UTC)\nAll lubridate functions respect the tz of dttm\nDates do NOT have timezones. Need to think carefully if converting a dttm column to date class\nWe do not need to name variables per their timezone b/c tz is preserved in the units (see exception for qualtrics)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exploring date-times and dates with lubridate</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Data Wrangling and Visualization in the Tidyverse",
    "section": "",
    "text": "https://jhudatascience.org/tidyversecourse/wrangle-data.html",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>resources.html</span>"
    ]
  }
]